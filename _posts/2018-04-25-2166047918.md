---
layout: post
title: 基于像素的乒乓游戏
categories:
- Pocket
tags:
---
原文地址：https://blog.csdn.net/lishuandao/article/details/52694770

收藏时间：2018-04-25 10:22:26

<div  >





<div id="RIL_IMG_1" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/1"/></div>      




<p nodeIndex="30"><span nodeIndex="347">乒乓游戏是研究简单强化学习的一个非常好的例子。在ATARI 2600中，我们将让你控制其中一个球拍，另一个球拍则由AI控制。游戏是这么工作的：我们获得一帧图像(210*160*3的字节数组)，然后决定将球拍往上还是往下移动(2种选择)。每次选择之后，游戏模拟器会执行相应的动作并返回一个回报(得分)：<span nodeIndex="384">可能是一个+1(如果对方没有接住球)，或者-1(我们没有接住球)，或者为0(其他情况)。</span>当然，<span nodeIndex="385">我们的目标是移动球拍，使得我们的得分尽可能高。</span></span><div id="RIL_IMG_2" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/2"/></div><div id="RIL_IMG_3" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/3"/></div></p>

<span nodeIndex="386">当我们描述解决方案的时候，请记住我们会尽量减少对于乒乓游戏本身的假设。因为我们更关心复杂、高维的问题，比如机器人操作、装配以及导航。乒乓游戏只是一个非常简单的测试用例，当我们学会它的同时，也将了解如何写一个能够完成任何实际任务的通用AI系统。</span>

<h3 nodeIndex="33"><span nodeIndex="387"><span nodeIndex="388">策略网络</span></span></h3>
<p nodeIndex="34"><span nodeIndex="389">首先，我们将定义一个策略网络，用于实现我们的玩家(智能体)。该网络根据当前游戏的状态来判断应该执行的动作(上移或者下移)。简单起见，我们使用一个2层的神经网络，以当前游戏画面的像素(100,800维，210*160*3)作为输入，输出一个单独的值表示“上移”动作的概率。在每轮迭代中，<span nodeIndex="390">我们会根据该分布进行采样，得到实际执行的动作</span>。这么做的原因会在接下来介绍训练过程的时候更加清楚。</span><div id="RIL_IMG_4" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/4"/></div></p>

<span nodeIndex="391">我们将它具体化，下面是我们如何使用Python/numpy来实现该策略网络。假设输入为x，即(预处理过的)像素信息：</span>
<p nodeIndex="37"><span nodeIndex="348">在这段简短的代码中，<span nodeIndex="393">W1与W2是两个随机初始化的权值矩阵</span>。我们<span nodeIndex="394">使用sigmoid函数作为非线性激活函数，以获得概率值([0,1])</span>。<span nodeIndex="395">直觉上，隐含层神经元会检测游戏的当前状态(比如：球与球拍的位置)，而W2则根据隐含层状态来决定当前应该执行的动作。</span>因此，W1与W2将成为决定我们玩家水平的关键因素。</span></p>
<p nodeIndex="38"><span nodeIndex="396">补充：之所以提到预处理，是因为我们通常需要2帧以上的画面作为策略网络的输入，这样策略网络才能够检测当前球的运动状态。由于我是在自己的Macbook上进行实验的，<span nodeIndex="397">所以我只考虑非常简单的预处理：实际上我们将当前帧与上一帧画面的差作为输入</span>。</span></p>
<h3 nodeIndex="39"><span nodeIndex="398"><span nodeIndex="399">这听起来并不现实</span></span></h3>
<p nodeIndex="40"><span nodeIndex="349">我想你已经体会到这个任务的难度。每一步，我们取得100,800个值作为输入，并传递给策略网络(W1、W2的参数数目将为百万级别)。假设我们决定往上移动球拍，当前的游戏可能给我们反馈的回报是0，并呈现下一帧画面(同样是100,800个值)。<span nodeIndex="400">我们重复这个过程上百次，最终得到一个非零的回报(得分)。</span>假<span nodeIndex="401">设我们最后得到的回报是+1，非常棒，但是我们如何知道赢在哪一步呢?</span>是取得胜利之前的那一步?还是在76步之前?又或许，它和第10步与第90步有关?同时，我们又该更新哪些参数，如何更新，使得它在未来能够做得更好?<span nodeIndex="402">我们将它称之为信用分配问题(credit assignment)</span>。在乒乓游戏的例子里，当对方没有接住球的时候，我们会得到+1的回报。<span nodeIndex="403">其直接原因是我们恰好将球以一个巧妙的轨迹弹出，但实际上，我们在很多步之前就已经这么做了，只不过那之后的所有动作都不直接导致我们结束游戏并获得回报。由此可见，我们正面临一个多么困难的问题。</span></span></p>
<h3 nodeIndex="41"><span nodeIndex="404"><span nodeIndex="405">有监督学习</span></span></h3>
<p nodeIndex="42"><span nodeIndex="350">在我们深入介绍策略梯度之前，我想简单回顾一下有监督学习(supervised learning)，因为我们将会看到，<span nodeIndex="406">强化学习与之很相似。</span>参考下图。在一般的有监督学习中，我们将一幅图片作为网络的输入，并获得某种概率分布，如：上移(UP)与下移(DOWN)两个类别。在这个例子中，我给出了这两种类别的对数概率(log probabilities)(-1.2, -0.36)(<span nodeIndex="407">我们没有列出原始的概率：0.3/0.7，因为我们始终是以正确类别的对数似然作为优化目标的)</span>。在有监督学习中，我们已经知道每一步应该执行的动作。假设当前状态下，正确的动作是往上移(label 0)。那么在实现时，我们会先给UP的对数概率一个1.0的梯度，然后执行反向传播计算梯度向量。梯度值将指导我们如何更新相应的参数，使得网络更倾向于预测UP这个动作。<span nodeIndex="408">当我们更新参数之后，我们的网络将在未来遇到相似输入画面时更倾向预测UP动作。</span></span><div id="RIL_IMG_5" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/5"/></div></p>

<h3 nodeIndex="44"><span nodeIndex="410"><span nodeIndex="411">策略梯度</span></span></h3>
<p nodeIndex="45"><span nodeIndex="351">但是，<span nodeIndex="412">在强化学习中我们并没有正确的标签信息，又该怎么做呢?</span>下面介绍策略梯度方法。我们的策略网络预测上移(UP)的概率为30%，下移(DOWN)的概率为70%。<span nodeIndex="413">我们将根据该分布采样出一个动作(如DOWN)并执行(Monte Carlo Method)。</span>注意到一个有趣的事实，我们可以像在有监督学习中所做的，立刻给DOWN一个梯度1.0，再进行反向传播更新网络参数，使得网络在未来更倾向预测DOWN。<span nodeIndex="414">然而问题是，在当前状态下，我们并不知道DOWN是一个好的动作。</span><span nodeIndex="415">但是没关系，我们可以等着瞧!比如在乒乓游戏中，我们可以等到游戏结束，获得游戏的回报(胜：+1;负：-1)，再对我们所执行过的动作赋予梯度(DOWN)(Monte Carlo Search)。</span>在下面这个例子中，下移(DOWN)的后果是导致我们输了游戏(回报为-1)，因此我们给DOWN的对数概率-1的梯度，再执行反向传播，使得网络在遇到相似输入的时候预测DOWN的概率降低。</span><div id="RIL_IMG_6" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/6"/></div></p>

<span nodeIndex="352"><span nodeIndex="416">就这么简单。我们有一个随机的策略对动作进行采样，对于那些最终能够带来胜利的动作进行激励，而抑制那些最终导致游戏失败的动作(</span><span nodeIndex="417">Monte Carlo Search</span><span nodeIndex="418">)。</span>同时，<span nodeIndex="419">回报并不限制为+1/-1，它可以是任何度量。假如确实赢得漂亮，回报可以为10.0(引入经验，先验知识_Prior Knowledge)，</span>我们将该回报值写进梯度再进行反向传播。这就是神经网络的美妙之处，使用它们的时候就像是一种骗术，你能够对百万个参数执行万亿次浮点运算，用<span nodeIndex="420">SGD(Stochastic Gradient Descent，随机梯度下降算法)</span>来让它做任何事情。它本不该行得通，但有趣的是，在我们生活的这个宇宙里，它确实是有效的。</span>

<h3 nodeIndex="48"><span nodeIndex="421"><span nodeIndex="422">训练准则</span></span></h3>
<p nodeIndex="49"><span nodeIndex="353">接下来，我们详细介绍训练过程。我们首先对策略网络中的W1, W2进行初始化，然后玩100局乒乓游戏(<span nodeIndex="423">我们把最初的策略称为rollouts，这时的策略是随机初始化的</span>)。假设每一次游戏包含200帧画面，那么我们一共作了20,000次UP/DOWN的决策，而且对于每一次决策，我们知道当前参数的梯度。剩下的工作就是为每个决策标注出其“好”或“坏”。假设我们赢了12局，输了88局，那么我们将对于那些胜局中200*12=2400个决策进行<span nodeIndex="424">正向更新(positive update，为相应的动作赋予+1.0的梯度，并反向传播，更新参数)</span>;而对败局中200*88=17600个决策进行负向更新。是的，就这么简单。<span nodeIndex="425">更新完成之后的策略网络将会更倾向执行那些能够带来胜利的动作。于是，我们可以使用改进之后的策略网络再玩100局，循环往复。</span></span></p>
<p nodeIndex="50"><span nodeIndex="426"><span nodeIndex="427">策略梯度：<span nodeIndex="428">对于一个策略执行多次，观察哪些动作会带来高回报，提高执行它们的概率。</span><div id="RIL_IMG_7" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/7"/></div></span></span></p>

<span nodeIndex="354">如果你仔细思考这个过程，<span nodeIndex="429">你会发现一些有趣的性质</span>。比如，<span nodeIndex="430">当我们在第50帧的时候作了一个好的动作(正确地回球)，但是在第150帧没有接住球，将会如何?按照我们所描述的做法，这一局中所有动作都将被标记为“坏”的动作(因为我们输了)，而这样不是也否定了我们在第50帧时所作出的正确决策么?你是对的——它会这样做</span>。然而，<span nodeIndex="431">如果你考虑上百万次游戏过程，一个正确的动作给你带来胜利的概率仍然是更大的，也就意味着你将会观察到更多的正向更新，从而使得你的策略依然会朝着正确的方向改进（这里否定这个正确动作）</span>。</span>

<h3 nodeIndex="53"><span nodeIndex="432"><span nodeIndex="433">更一般的回报函数</span></span></h3>
<p nodeIndex="54"><span nodeIndex="434">到目前为止，<span nodeIndex="435">我们都是通过是否赢得游戏来判断每一步决策的好坏</span>。在更一般的强化学习框架中，我们可能在每一步决策时都会获得一些回报rt。一个常见的选择是采用有折扣的回报(discounted reward)，这样一来，“最终回报”则为:</span></p>
<p nodeIndex="55"><span class="math" id="MathJax-Span-24" nodeIndex="436"><span nodeIndex="437"><span nodeIndex="438"><span class="mrow" id="MathJax-Span-25" nodeIndex="439"><span class="msubsup" id="MathJax-Span-26" nodeIndex="440"><span nodeIndex="441"><span nodeIndex="442"><span class="mi" id="MathJax-Span-27" nodeIndex="443">R</span></span><span nodeIndex="444"><span class="mi" id="MathJax-Span-28" nodeIndex="445">t</span></span></span></span><span class="mo" id="MathJax-Span-29" nodeIndex="446">=</span><span class="munderover" id="MathJax-Span-30" nodeIndex="447"><span nodeIndex="448"><span nodeIndex="449"><span class="mo" id="MathJax-Span-31" nodeIndex="450">∑</span></span><span nodeIndex="451"><span class="texatom" id="MathJax-Span-32" nodeIndex="452"><span class="mrow" id="MathJax-Span-33" nodeIndex="453"><span class="mi" id="MathJax-Span-34" nodeIndex="454">∞</span></span></span></span><span nodeIndex="455"><span class="texatom" id="MathJax-Span-35" nodeIndex="456"><span class="mrow" id="MathJax-Span-36" nodeIndex="457"><span class="mi" id="MathJax-Span-37" nodeIndex="458">k</span><span class="mo" id="MathJax-Span-38" nodeIndex="459">=</span><span class="mn" id="MathJax-Span-39" nodeIndex="460">0</span></span></span></span></span></span><span class="msubsup" id="MathJax-Span-40" nodeIndex="461"><span nodeIndex="462"><span nodeIndex="463"><span class="mi" id="MathJax-Span-41" nodeIndex="464">γ</span></span><span nodeIndex="465"><span class="mi" id="MathJax-Span-42" nodeIndex="466">k</span></span></span></span><span class="msubsup" id="MathJax-Span-43" nodeIndex="467"><span nodeIndex="468"><span nodeIndex="469"><span class="mi" id="MathJax-Span-44" nodeIndex="470">r</span></span><span nodeIndex="471"><span class="texatom" id="MathJax-Span-45" nodeIndex="472"><span class="mrow" id="MathJax-Span-46" nodeIndex="473"><span class="mi" id="MathJax-Span-47" nodeIndex="474">t</span><span class="mo" id="MathJax-Span-48" nodeIndex="475">+</span><span class="mi" id="MathJax-Span-49" nodeIndex="476">k</span></span></span></span></span></span></span></span></span></span>      <br nodeIndex="477"></p>
<span nodeIndex="355">，其中γ为折扣因子(0到1之间)。<span nodeIndex="478">这意味着对于每一步所采样的动作，我们考虑未来所有回报的加权和，而且未来回报的重要性随时间以指数递减。</span>在实际中，对<span nodeIndex="479">回报值进行规范化也很重要</span>。比如，当我们算完100局乒乓游戏中20,000个动作的回报Rt之后，一个好的做法是<span nodeIndex="480">先对这些回报值进行规范化(standardize，如减去均值，除标准差)</span>再进行反向传播。<span nodeIndex="481">这样一来，被激励以及被抑制的动作大概各占一半（<span nodeIndex="482">为什么这样规范化?</span>）。</span>从数学角度，我们可以将这种做法看成是一种控制策略梯度估计方差的一种方法。更深入的探讨请参考论文：High-Dimensional Continuous Control Using Generalized Advantage Estimation。</span>

<h3 nodeIndex="57"><span nodeIndex="483"><span nodeIndex="484">策略梯度的推导</span></span></h3>
<p nodeIndex="58"><span nodeIndex="485">接下来，我将从<span nodeIndex="486">数学角度简要介绍策略梯度的推导过程</span>。策略梯度是值函数梯度估计的一个特例。更一般的情况下，表达式满足这样的形式:</span></p>
<p nodeIndex="59"><span class="msubsup" id="MathJax-Span-60" nodeIndex="487"><span nodeIndex="488"><span nodeIndex="489"><span class="mi" id="MathJax-Span-61" nodeIndex="490">E</span></span><span nodeIndex="491"><span class="texatom" id="MathJax-Span-62" nodeIndex="492"><span class="mrow" id="MathJax-Span-63" nodeIndex="493"><span class="mi" id="MathJax-Span-64" nodeIndex="494">x</span><span class="mo" id="MathJax-Span-65" nodeIndex="495">∼</span><span class="mi" id="MathJax-Span-66" nodeIndex="496">p</span><span class="mo" id="MathJax-Span-67" nodeIndex="497">(</span><span class="mi" id="MathJax-Span-68" nodeIndex="498">x</span><span class="mo" id="MathJax-Span-69" nodeIndex="499">∣</span><span class="mi" id="MathJax-Span-70" nodeIndex="500">θ</span><span class="mo" id="MathJax-Span-71" nodeIndex="501">)</span></span></span></span></span></span><span class="mi" id="MathJax-Span-73" nodeIndex="502"><span nodeIndex="503"> </span></span><span class="mi" nodeIndex="504">f</span><span class="mo" id="MathJax-Span-74" nodeIndex="505">(</span><span class="mi" id="MathJax-Span-75" nodeIndex="506">x</span><span class="mo" id="MathJax-Span-76" nodeIndex="507">)</span></p>
<span nodeIndex="508">，即某个值函数f(x)在概率分布p(x;θ)下的期望。<span nodeIndex="509">提示：f(x)将是我们的回报函数(或者advantage function)</span>，<span nodeIndex="510">而p(x)是我们的策略网络，实际上是p(a|I)，即在给定图像I的条件下，动作的概率分布</span>。我们感兴趣的是如何调整该分布(通过它的参数θ)以提高样本的期望回报。</span><div id="RIL_IMG_8" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/8"/></div>

<span nodeIndex="356">首先，我们有一个用以动作采样的分布p(x;θ)(比如，可以是一个高斯分布)。<span nodeIndex="511">对于每一个采样，我们通过估值函数f对其进行打分</span>。<span nodeIndex="512">上面的式子告诉我们如何更新这个分布，使得我们所采样的动作能够获得更高的分数</span>。更具体一点，我们采样出一些样本(动作)x，获得它们的得分f(x)，同时对于x中的每一个样本计算式中的第二项：</span><div id="RIL_IMG_9" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/9"/></div>
<p nodeIndex="63"><span nodeIndex="514">这一项代表什么含义?它是一个向量，指向能够使得样本x 概率增加的参数更新方向。<span nodeIndex="515">回到上面的式子，最终的梯度是该向量与样本得分f(x)的乘积，从而使得得分更高的样本比得分较低的样本对分布p(x;θ)的影响更大。</span></span><div id="RIL_IMG_10" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/10"/></div></p>

<span nodeIndex="516">我希望策略网络与强化学习之间的联系已经介绍清楚了。策略网络为我们提供一些动作的样本，并且其中一些动作比另外一些得分(由估值函数给出)更高。这部分所谈及的数学部分则指导我们<span nodeIndex="517">如何更新策略网络的参数</span>：计算相应动作的梯度，乘上它的得分，再更新网络参数。一个更全面的推导过程以及讨论，推荐大家看看John Sculman的讲座。</span>

<h3 nodeIndex="66"><span nodeIndex="518"><span nodeIndex="519">学习</span></span></h3>
<p nodeIndex="67"><span nodeIndex="520">好，我们已经了解了策略梯度以及它的推导过程。我在OpenAI Gym项目中ATARI 2600 Pong的基础之上实现了整个方法，一共是130行Python代码。我训练了一个<span nodeIndex="521">两层的策略网络</span>，<span nodeIndex="522">隐层200个神经元</span>，使用<span nodeIndex="523">RMSProp(这个算法需要去看一下？)</span>算法更新参数。我没有进行太多地调参(<span nodeIndex="524"><a href="http://blog.csdn.net/xiewenbo/article/details/51585054" nodeIndex="525">超参：什么是超参数</a></span>)，实验在我的Macbook上跑的。在<span nodeIndex="526">训练了3晚上之后（增强学习的训练时间一般都会多久，在自己点电脑上？）</span>，我得到了一个比AI玩家稍好的策略。这个算法大概模拟了<span nodeIndex="527">200,000局游戏</span>，执行了800次左右参数更新。我朋友告诉我，如果使用ConvNet在GPU上跑几天的话，你的胜率会更高。而如果你再优化一下超参数，你就可以完胜每一局游戏。但是我在这篇文章里只是为了展示核心的思想，并没有花太多时间去调整模型，所以最终得到的只是一个表现还不错的乒乓游戏AI。</span></p>
<p nodeIndex="68"><span nodeIndex="528"><span nodeIndex="529">学到的(网络)权重</span></span></p>
<p nodeIndex="69"><span nodeIndex="530">我们来观察一下所学到的网络权重(weights)。经预处理之后，网络的输入是一个80*80的差值图像(当前帧减去上一帧，以反映图像变化轨迹)。我们将W1按行进行展开(每一行的维度则为80*80)并可视化。下面展示了200个隐层神经元中的40个。白色像素表示权值为正，黑色为负。可以看到，有一些神经元显示了球的运动轨迹(黑白相间的线条)。<span nodeIndex="531">由于每个时刻球只能处在一个位置，所以这些神经元实际上在进行多任务处理(multitasking)，并在球出现在相应位置时被充分激活。</span>当球沿着相应的轨迹运动时，该神经元的响应将呈现出正弦波形的波动，而由于ReLU的影响，它只会在一些离散的位置被激活。画面中有一些噪声，我想可以通过L2正则来消解。</span><div id="RIL_IMG_11" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/11"/></div></p>


<h3 nodeIndex="72"><span nodeIndex="532"><span nodeIndex="533">What isn’t happening</span></span></h3>
<p nodeIndex="73"><span nodeIndex="357">策略梯度算法是以一种巧妙的“<span nodeIndex="534">guess-and-check（trial-and-error）</span>”模式工作的，“guess”指的是我们每一次都根据当前策略来对动作进行采样，“check”指的是我们不断激励那些能够带来好运(赢或者得分更高)的动作。<span nodeIndex="535">这种算法是目前解决强化学习问题最好的方法，但是一旦你理解了它的工作方式，你多少会感到有点失望，并心生疑问——它在什么情况下会失效呢?</span></span></p>
<p nodeIndex="74"><span nodeIndex="536">与人类学习玩乒乓游戏的方式相比，我们需要注意以下区别：</span></p>
<ul nodeIndex="76"><li nodeIndex="75"><span nodeIndex="537">在实际场景中，人类通常以某种方式(比如：英语)对这个任务进行交流，但是在一个标准的强化学习问题中，我们只有与环境交互而产生的回报。假如人类在对回报函数一无所知的情况下学习玩这个游戏(尤其是当回报函数是某种静态但是随机的函数)，那么他也许会遇到非常多的困难，而此时策略梯度的方法则会有效得多。类似的，假如我们把每一帧画面随机打乱，那么人类很可能会输掉游戏，而对于策略梯度法却不会有什么影响。</span></li>
<li nodeIndex="77"><span nodeIndex="538">人类有丰富的先验知识，比如物理相关(球的反弹，它不会瞬移，也不会忽然停止，它是匀速运动，等)与心理直觉。而且你也知道球拍是由你控制的，会对你的“上移/下移”指令作出回应。而我们的算法则是完全从一无所知的状态开始学习。</span></li>
<li nodeIndex="78"><span nodeIndex="539"><span nodeIndex="540">策略梯度是一种brute force(暴力搜索)算法。正确的动作最终会被发现并被内化至策略当中。人类则会构建一个强大的抽象模型，并根据它来作出规划。</span>在乒乓游戏里，我能够推断出对手速度很慢，因此将球快速击出可能是一个好的策略。然而，似乎我们最终同样会把这些好的策略内化至我们的肌肉记忆，从而作出反射式操作。比如，当我们在学习开手动档车的时候，你会发现自己在一开始需要很多思考，但是最终会成为一种自动而下意识的操作。</span></li>
<li nodeIndex="79"><span nodeIndex="358"><span nodeIndex="541">采用策略梯度时需要不断地从正回报样本中进行学习，从而调整参数。而在人类进行学习的过程中，即使没有经历过相应的选择(样本)，也能够判断出哪个动作会有较高的回报。</span>比如，<span nodeIndex="542">我并不需要上百次的撞墙经历才能够学会在开车时如何避免(撞墙)——这个例子举得感觉很好。</span></span><div id="RIL_IMG_12" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/12"/></div></li>
</ul>
<span nodeIndex="359">综上所述，<span nodeIndex="543">一旦你理解了这些算法工作的原理，你就可以推断出它们的优势以及不足。特别地，我们离像人类一样通过构建抽象而丰富的游戏表示以进行快速地规划和学习还差得很远。</span><span nodeIndex="544">未来的某天，计算机将会从一组像素中观察到一把钥匙和一扇门，并且会想要拿起钥匙去开门。目前为止，我们离这个目标还有很长的距离，而且这也是相关研究的热点领域。</span></span>

<h3 nodeIndex="82"><span nodeIndex="545"><span nodeIndex="546">神经网络中的不可微计算</span></span></h3>
<p nodeIndex="83"><span nodeIndex="547">这一节将讨论策略梯度的另一个有趣的应用，与游戏无关：它允许我们设计、训练含有不可微计算的神经网络。这种思想最早在论文“Recurrent Models of visual Attention”中提出。这篇论文主要研究低分辨率图像序列的处理。在每一轮迭代中，RNN会接收图片的某一部分作为输入，然后采样出下个区域的位置。例如，RNN可能先看到(5, 30)这个位置，获得该区域的图片信息，然后决定接下来看(24, 50)这个位置。这种方法的关键在于从当前图片(局部)中产生下一个观察位置的概率分布并进行采样。不幸地是，这个操作是不可微的。更一般地，我们考虑下面的神经网络：</span><div id="RIL_IMG_13" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/13"/></div></p>

<span nodeIndex="548">注意到大部分箭头(蓝色)是连续可微的，但是其中某些过程可能会包含不可微的采样操作(红色)。对于蓝箭头我们可以轻易地进行反向传播，而对于红色箭头则不能。</span>

<p nodeIndex="86"><span nodeIndex="549">使用策略梯度可以解决这个问题!我们将网络中的采样过程视为嵌入在网络中的一个随机策略。那么，在训练时，我们将产生不同的样本(下图中的分支)，然后我们在参数更新过程中对那些最终能够得到好结果(最终的loss小)的采样进行激励。换言之，对于蓝色箭头上的参数，我们依然按照一般的反向传播来更新;而对于红色箭头上的参数，我们采用策略梯度法更新。论文：Gradient Estimation Using Stochastic Computation Graphs对这种思路作了很好地形式化描述。</span><div id="RIL_IMG_14" class="RIL_IMG"><img src="/media/posts_images/2018-04-25-2166047918/14"/></div></p>

<span nodeIndex="550">可训练的记忆读写。你会在很多其他论文中看到这种想法，例如：Neural Turing Machine(NTM)有一个可读写的存储带。在执行写操作时，我们可能执行的是类似m[i]=x的操作，其中i和x是由RNN控制器预测的。但是这个操作并不可微，因为我们并不知道如果我们写的是一个不同的位置 j!=i 时loss会有什么样的变化。因此，NTM只好执行一种“软”的读/写操作：它首先预测一个注意力的分布(attention distribution)a(按可写的位置进行归一化)，然后执行：for all i: m[i] = a[i]*x。是的，这样做的确可微了，但是计算代价也高了很多。想象一下，我们每次赋值都需要访问整个内存空间!</span>

<p nodeIndex="89"><span nodeIndex="551">然而，我们可以使用策略梯度来避免这个问题(理论上)，正如RL-NTM所做的(arxiv.org/abs/1505.00521)。我们仍然预测一个注意力的分布 a，再根据a 对将要写的地址 i 进行采样：i = sample(a); m[i] = x。在训练过程中，我们会对 i 采样多次，并通过策略梯度法来对那些最好的采样进行激励。这样一来，在预测时就只需要对一个位置进行读/写操作了。但是，正如这篇文章(RL-NTM)所指出的，策略梯度算法更适用于采样空间较小的情况，而在这种搜索空间巨大的任务中，策略梯度并不是很有效。</span></p>
<h3 nodeIndex="90"><span nodeIndex="552"><span nodeIndex="553">结论</span></span></h3>
<p nodeIndex="91"><span nodeIndex="554">我们已经看到策略梯度是一种强大且通用的算法。我们以ATARI 乒乓游戏为例，使用130行Python代码训练得到了一个智能体。同样的算法也适用于其他的任何游戏，甚至在未来可用于有实际价值的控制问题。在掷笔之前，我想再补充一些说明：</span></p>
<p nodeIndex="92"><span nodeIndex="360"><span nodeIndex="555">关于推进人工智能。我们看到策略梯度算法是通过一个brute-force搜索的过程来完成的</span>。我们同样看到<span nodeIndex="556">人类用以解决同样问题的方法截然不同</span>。正是<span nodeIndex="557">由于人类所使用的那些抽象模型非常难以(甚至不可能)进行显式地标注，所以最近越来越多的学者对于(无指导)生成模型(generative models)以及程序归纳(program induction)产生了兴趣。</span></span></p>
<p nodeIndex="93"><span nodeIndex="361">在复杂机器人场景中的应用。将<span nodeIndex="558">策略梯度算法应用到搜索空间巨大的场景中并非易事</span>，比如机器人控制(一个或多个机器人实时与现实世界进行交互)。解决这个问题的一个相关研究思路是：确定性的策略梯度(deterministic policy gradients)，这种方法采用一个确定性的策略，并从一个额外的估值网络(称为critic)中获取梯度信息。<span nodeIndex="559">另一种思路是增加额外的指导</span>。在很多实际的情形中，我们能够从人类玩家那儿获取一些专业的指导信息。<span nodeIndex="560">比如AlphaGo先是根据人类玩家的棋谱，使用有监督学习得到能够较好地预测人类落子方式的策略，然后再使用策略梯度算法对此策略进行调整。</span></span></p>
<p nodeIndex="94"><span nodeIndex="362">策略梯度在实际中的使用。<span nodeIndex="561">在我之前关于RNN的博文中，我可能已经让大家看到了RNN的魔力</span>。而事实上<span nodeIndex="562">让这些模型有效地工作是需要很多小技巧(trick)支撑的</span>。策略梯度法也是如此。它是自动的，你需要大量的样本，它可以一直学下去，当它效果不好的时候却很难调试。无论什么时候，当我们使用火箭炮之前必须先试试空气枪。<span nodeIndex="563">以强化学习为例，我们在任何时候都需要首先尝试的一个(强)基线系统是：交叉熵</span>。<span nodeIndex="564">假如你坚持要在你的问题中使用策略梯度，请一定要注意那些论文中所提到的小技巧，从简单的开始，并使用策略梯度的的一种变型：TRPO。TRPO在实际应用中几乎总是优于基本的策略梯度方法。其核心思想是通过引入旧策略和更新之后策略所预测的概率分布之间的KL距离，来控制参数更新的过程。</span></span></p>
<p nodeIndex="95"><span nodeIndex="565">via:哈工SCIR</span></p>
</div>