---
layout: post
title: "Google海量文本去重算法SimHash"
categories:
- 今日头条
tags:
---
青年极客微信公众号：youthgeek，和我一起在极客的世界里遨游。 

 前俩篇文章 （网络爬虫实战（一）和 网络爬虫实战（二））中分别讲述了爬虫的基本概念和结合实际需求的应用场景分析，通过爬虫系统，我们采集了大量文本数据，但是文本中有很多重复数据影响我们对于结果的分析。分析前我们需要对这些数据去除重复，而如何选择和设计文本的去重算法？常见的有余弦夹角算法、欧式距离、Jaccard相似度、最长公共子串、编辑距离等。这些算法对于待比较的文本数据不多时还比较好用，如果我们的爬虫每天采集的数据以千万计算，我们如何对于这些海量千万级的数据进行高效的合并去重。

![](http://p3.pstatp.com/large/c5e0000c75d48872d5a)

 SimHash算法提供了很好的解决方案，SimHash是Google用来处理海量文本去重的算法。 SimHash最牛逼的一点就是将一个文档，最后转换成一个64位的字节，暂且称之为特征字，然后判断重复只需要判断他们的特征字的距离是不是<n（根据经验这个n一般取值为3），就可以判断两个文档是否相似。

原理SimHash的基本原理如下：

![](http://p1.pstatp.com/large/c5d0003ec9f49f5696b)

大概花三分钟看懂这个图就差不多怎么实现这个SimHash算法了。特别简单。

算法过程大概如下：

分词：将Doc进行关键词抽取(其中包括分词和计算权重)（也可以仅进行简单分词，那么所有词的权重都为1），抽取出n个(关键词，权重)对， 即图中的(feature, weight)hash：将上面生成的关键词通过hash算法把关键词变成hash值，记为：（hash,weight）加权：然后对上一步生成的（hash,weight）按照单词的权重形成加权数字串，如果该位是1，则+weight,如果是0，则-weight，则生成关键词的加权数字串合并：将所有关键词的加权数字串进行位的纵向累加，最后生成bits_count个数字，如图所示是[13, 108, -22, -5, -32, 55]降维：[13,108,-22,-5,-32,55] -> 110001这个就很简单啦，正1负0。形成我们最终的SimHash签名。

到此，如何从一个Doc到一个SimHash值的过程已经讲明白了。

SimHash值的海明距离计算

 二进制串A 和 二进制串B 的海明距离 就是 A xor B 后二进制中1的个数。

 举例如下：

 A = 100111; B = 101010; hamming_distance(A, B) = count_1(A xor B) = count_1(001101) = 3;

 当我们算出所有doc的SimHash值之后，需要计算doc A和doc B之间是否相似的条件是：

 A和B的海明距离是否小于等于n，这个n值根据经验一般取值为3,

 SimHash本质上是局部敏感性的hash，和md5之类的不一样。 正因为它的局部敏感性，所以我们可以使用海明距离来衡量SimHash值的相似度。

实际应用

 对于爬虫采集的文档数据，我们可以先使用SimHash算法算出该文档的SimHash值，并在内存中或者数据库中进行存储，当有新的文档数据产生时，只需要将新文档的SimHash值和内存中的所有SimHash值进行计算海明距离，设置海明距离允许的大小来设置俩个文档相似度的匹配程度。