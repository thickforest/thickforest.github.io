---
layout: post
title: Linux Multipath代码分析
categories:
- Pocket
tags:
---
原文地址：http://www.cnblogs.com/D-Tec/archive/2013/03/01/2938969.html

收藏时间：2014-10-30 09:08:17

<div  lang="en">
<p nodeIndex="32">DM-Multipathing (DM-MPIO) provides <strong nodeIndex="360">I/O failover and load-balancing</strong> within Linux for block devices. By utilizing device-mapper, multipathd provides the host-side logic to use multiple paths of a redundant network to provide continuous availability and higher bandwidth connectivity between the host server and the block-level device.</p>
<div id="RIL_IMG_1" class="RIL_IMG"><img src="/media/posts_images/2014-10-30-582225898/1"/></div>
<h1 nodeIndex="34">2 Multipath基本原理</h1>
<h2 nodeIndex="35">2.1 Device Mapper</h2>
<p nodeIndex="36">在内核中它通过一个一个模块化的 target driver 插件实现<strong nodeIndex="362">对</strong> <strong nodeIndex="363">IO</strong> <strong nodeIndex="364">请求的过滤或者重新定向</strong>等工作，当前已经实现的 target driver 插件包括软 raid、软加密、逻辑卷条带、多路径、镜像、快照等，图中 linear、mirror、snapshot、multipath 表示的就是这些 target driver。</p>
<p nodeIndex="37">Device mapper 进一步体现了在 Linux 内核设计中策略和机制分离的原则，将所有与策略相关的工作放到用户空间完成，内核中主要提供完成这些策略所需要的机制。Device mapper <strong nodeIndex="365">用户空间相关部分主要负责配置具体的策略和控制逻辑</strong>，比如逻辑设备和哪些物理设备建立映射，怎么建立这些映射关系等等，而<strong nodeIndex="366">具体过滤和重定向</strong> <strong nodeIndex="367">IO</strong> <strong nodeIndex="368">请求的工作由内核中相关代码完成</strong>。因此整个 device mapper 机制由两部分组成，内核空间的 device mapper 驱动、用户空间的device mapper 库以及它提供的 dmsetup 工具。</p>

<h3 nodeIndex="39">2.1.1 Device Mapper内核对象</h3>

<p nodeIndex="41"><strong nodeIndex="371">mapped_device:</strong> device mapper生成的<strong nodeIndex="372">逻辑块设备</strong>，map指向dm_table</p>
<p nodeIndex="42"><strong nodeIndex="373">dm_table:</strong> mapped_device和物理设备target的映射，按照B树组织dm_target，便于IO请求映射的查找</p>
<p nodeIndex="43"><strong nodeIndex="374">dm_target:</strong> 记录映射的mapped device的逻辑区域的开始地址和范围，type指向target设备的驱动类型（target_type）,private记录具体设备的数据。</p>
<p nodeIndex="44"><strong nodeIndex="375">target_type</strong> multipath_target: mulitpath类型的target的driver,包括创建、销毁device的方法。</p>
<p nodeIndex="45">Device mapper本质功能就是根据映射关系和target driver描述的IO处理规则，将IO请求从逻辑设备mapped device转发相应的target device上。Device mapper处理所有从内核中块一级IO子系统的generic_make_request和submit_bio接口中定向到mapped device的所有块读写IO请求。IO请求在device mapper的设备树中通过请求转发从上到下地进行处理。当一个bio请求在设备树中的mapped deivce向下层转发时，一个或者多个bio的克隆被创建并发送给下层target device。target driver结束某个bio请求后，将表示结束该bio请求的事件上报给它上层的mapped device，该过程在各个层次上进行直到该事件最终上传到根mapped device的为止，然后device mapper结束根mapped device上原始bio请求，结束整个IO请求过程。</p>
<h3 nodeIndex="46">2.1.2 Device Mapper用户态功能</h3>
<p nodeIndex="47">（1） 发现每个mapped device相关的target device；</p>
<p nodeIndex="48">（2） 根据配置信息创建映射表；</p>
<p nodeIndex="49">（3） 将用户空间构建好的映射表传入内核，让内核构建该mapped device对应的dm_table结构；</p>
<p nodeIndex="50">（4） 保存当前的映射信息，以便未来重新构建。</p>
<h2 nodeIndex="51">2.2 Multipath</h2>
<p nodeIndex="52"><strong nodeIndex="376">The Device Mapper multipath module of the Linux kernel</strong>：multipath_target</p>
<p nodeIndex="53"><strong nodeIndex="377">The multipath-tools userspace package</strong><strong nodeIndex="378">：</strong>multipath-tools take care of <strong nodeIndex="379">automatic path discovery and grouping</strong> as well as automated <strong nodeIndex="380">path retesting</strong>, so that a previously failed path is automatically <strong nodeIndex="381">reinstated</strong> when it becomes healthy again. This minimizes the need for administrator attention in a production environment. <strong nodeIndex="382">Include Multipathd service and multipath(CLI)</strong></p>
<div id="RIL_IMG_2" class="RIL_IMG"><img src="/media/posts_images/2014-10-30-582225898/2"/></div>
<h3 nodeIndex="55">2.2.1 multipath_target</h3>
<p nodeIndex="56">主要提供的方法（每种类型的target都必须提供）：</p>
<p nodeIndex="57">multipath_ctr：构建target device 的方法</p>
<p nodeIndex="58">multipath_dtr：删除target device 的方法</p>
<p nodeIndex="59">multipath_map：<strong nodeIndex="384">Target</strong><strong nodeIndex="385">的映射IO</strong><strong nodeIndex="386">请求的方法，调用map_io</strong></p>
<p nodeIndex="60">multipath_status：获取当前target device状态的访问</p>
<p nodeIndex="61">multipath_message: Target 处理用户消息的方法，根据具体的消息类型会调用fail_path, queue_if_no_path, switch_pg_num等方法</p>
<p nodeIndex="62">multipath_end_io：Target结束IO请求的方法</p>
<h1 nodeIndex="63">3 Multipath实现分析</h1>
<h2 nodeIndex="64">3.1 对象</h2>
<p nodeIndex="65">Multipath</p>
<p nodeIndex="66">Pathgroup（优先组）</p>
<p nodeIndex="67">Path</p>
<p nodeIndex="68">配置文件中path_grouping_policy项如果设定为failover，则每个path都在不同的pathgroup，如果设置为multibus，则所有的path都在一个pathgroup。</p>
<h2 nodeIndex="69">3.2 配置</h2>
<p nodeIndex="70">path_grouping_policy</p>
<p nodeIndex="71">path_checker：指定用于决定路径状态的默认方法，包括：readsector0\directio</p>
<p nodeIndex="72">failback: 路径出错恢复正常后切换策略，包括immediate/manual/数字（代表时间）</p>
<p nodeIndex="73">no_path_retry：指定所有路径都失败后应该再试图尝试的次数。Queue/fail/数字（代表次数）</p>
<p nodeIndex="74">path_selector：指定用来决定下一个 I/O 操作所使用路径的默认算法。Round-robin/queue-length。</p>
<p nodeIndex="75">Prio: 指定要获得路径优先值所需调用的默认程序及参数。默认是const，即优先值都为1</p>
<p nodeIndex="76">Hardware_handler（device配置）：指定将在切换路径组群或者处理 I/O 错误时用来执行硬件具体动作的模块。配置值：emc/hp_sw/rdac。默认为”0”</p>
<h2 nodeIndex="77">3.3 事件机制</h2>
<p nodeIndex="78">The multipath daemon learns of and reacts to changes in both the current block device configuration and the kernel resident multipathing configuration.</p>
<p nodeIndex="79">The addition of a new path or the removal of an already existing path to an already managed block device is detected over a netlink socket as a <strong nodeIndex="387">uevent triggered callback</strong> which adds or removes the path to or from the set of paths which will be actively tested.</p>
<p nodeIndex="80">Changes to the kernel resident multipathing state are detected as <strong nodeIndex="388">device-mapper generated event callbacks</strong>. Events of this kind invole block io errors, path state change, and changes in the highest priority path group for a mapped device.</p>
<h3 nodeIndex="81">3.3.1 Device-mapper event</h3>
<p nodeIndex="82">内核Multipath对象的成员trigger_event，由Dm-mpath.c中的trigger_event函数赋值，</p>
<p nodeIndex="83">trigger_event调用Dm-table.c的dm_table_event函数，dm_table_event调用dm_table对象的event_fn，event_fn在Dm.c的dm_table_event_callback函数（<strong nodeIndex="389">对于该函数何时被调用还有疑问！！！</strong>）里被赋值为event_callback。</p>
<p nodeIndex="84">event_callback会wake_up mapped_device的eventq，而Dm_ioctl.c中dev_wait函数调用dm_wait_event函数等待eventq的事件中断。</p>
<p nodeIndex="85">用户态每个multipath对象都会启动一个waiter_thread，线程函数waitevent while(1)循环调用waiteventloop函数，waiteventloop最终调用内核的dev_wait方法。</p>
<h3 nodeIndex="86">3.3.2 Uevent</h3>
<p nodeIndex="87">用户态ueventloop线程监听Netlink events事件，调用uev_add_map/uev_add_path等方法处理map<strong nodeIndex="390">（未发现内核有此类事件！！！）</strong>和path相关的事件。</p>
<p nodeIndex="88">内核态dm_path_uevent（Dm_uevent.c）向Netlink events中加入path相关事件到mapped_device对象的uevent_list中。dm_send_uevents函数发送事件，该函数被event_callback方法调用。</p>
<h2 nodeIndex="89">3.4 路径发现</h2>
<h3 nodeIndex="90">3.4.1 自动发现</h3>
<p nodeIndex="91">Multipathd服务启动时会调用configure方法，该方法依次执行发现路径、过滤路径、发现multipath、通过Path生成Multipath、同步用户态和内核路径状态、启动Multipath的DM事件监听等操作。</p>
<p nodeIndex="92"><strong nodeIndex="391">发现路径</strong>：Path_discovery从/sys/block目录下获取设备名，即路径名，通过ioctl等方法获取设备信息。</p>
<p nodeIndex="93"><strong nodeIndex="392">过滤路径：</strong>filter_path根据配置文件中blacklist项的配置过滤路径。</p>
<p nodeIndex="94"><strong nodeIndex="393">发现multipath</strong>: 通过libdevmapper向内核DM发送DM_DEVICE_LIST指令，获取所有DM设备，保留multipath类型的设备，获取alias,size,params,status等信息，使用setup_multipath方法，首先从params中解析出multipath的部分配置（pathgroup/path）和状态，然后从配置文件（或者bulid-in）中获取mpe、hwe，再根据mpe、hwe的中配置multipath的rr_weight\pgfailback\no_path_retry等配置。</p>
<p nodeIndex="95">Multipathd第一次启动从DM获取multipath，返回应该为空。</p>
<p nodeIndex="96"><strong nodeIndex="394">通过Path</strong><strong nodeIndex="395">生成Multipath</strong>：coalesce_paths遍历所有路径，mpp=NULL的生成multipath对象，通过adopt_paths将wwid相同的其它路径加入到multipath的paths中。verify_paths检查multipath中所有的path是否都能从sysfs获取设备信息，如果不行删除。setup_map通过mpe/hwe/conf设置multipath配置，根据pgpolicy从path生成pathgroup<strong nodeIndex="396">（路径分组）</strong>，从配置打包params。select_action确定要对dm要执行的操作（create/rename），domap向内核DM层发送相应指令，DM创建相应的multipath_target。通过dm_message设置no_path_retry。删除deadmap(所有path都not in sysfs的Multipath)</p>
<p nodeIndex="97"><strong nodeIndex="397">同步用户态和内核路径状态</strong>： 遍历所有multipath的所有path，根据dmstate和state的差别，调用dm_reinstate_path或dm_fail_path向DM发送对应message，使内核管理的path状态和用户态一致。</p>
<p nodeIndex="98"><strong nodeIndex="398">启动DM</strong><strong nodeIndex="399">事件监听</strong>：用户态每个multipath对象启动一个监听线程，监听来自内核DM的报告的事件，可能包括路径错误等事件</p>
<h3 nodeIndex="99">3.4.2 手动添加</h3>
<p nodeIndex="100">使用Multipathd –k命令进去CLI管理界面，add path或add map可以添加路径和multipath</p>
<h2 nodeIndex="101">3.5 路径检查</h2>
<h3 nodeIndex="102">3.5.1 路径状态</h3>
<p nodeIndex="103">PATH_UNCHECKED: checker方法获取失败，或checker_init调用失败返回。</p>
<p nodeIndex="104">PATH_WILD： sysfs接口查询设备及状态失败时返回。</p>
<p nodeIndex="105">PATH_PENDING：从sysfs获取的设备状态是”blocked”。</p>
<p nodeIndex="106">PATH_DOWN：从sysfs获取的设备状态是”offline”，或通过checker方法检查返回。</p>
<p nodeIndex="107">PATH_UP：从sysfs获取的设备状态是”running”，并且通过了checker方法的检查。</p>
<p nodeIndex="108">PATH_SHAKY：emc_clariion检查方法中出现，类似于PATH_DOWN</p>
<p nodeIndex="109">PATH_GHOST: hp_sw和tur检查方法中出现，类似于PATH_UP</p>
<h3 nodeIndex="110">3.5.2 主动检查</h3>
<p nodeIndex="111">路径的初始检查间隔(checkint)由配置文件polling_interval项配置，默认为5。</p>
<p nodeIndex="112">配置文件中path_checker项设置路径检查方法，譬如readsector0，即通过ioctl读取设备的第0个扇区（长度根据fstat设备的属性，测试的iscsi是4K）。</p>
<p nodeIndex="113">Multipathd服务启动Checkerloop线程进行路径检查，遍历vecs中所有的path，调用check_path方法进行检查。</p>
<p nodeIndex="114">Check_path:</p>
<p nodeIndex="115">（1） getstate: 首先用path_offline通过sysfs获取设备状态(sysfs_get_state)，然后针对up状态的path再调用path_checker具体方法进行检查。</p>
<p nodeIndex="116">（2） 如果<strong nodeIndex="400">新旧状态不一致</strong>，作如下（3）-（6）处理：</p>
<p nodeIndex="117">（3） 恢复path的检查间隔checkint为配置值(conf->checkint)</p>
<p nodeIndex="118">（4） 如果新状态是fail或shark，update_multipath_strings，通过dm_message主动通知DM，并且如果旧状态是up或ghost，减少该multipath的活跃路径数nr_active。</p>
<p nodeIndex="119"><strong nodeIndex="401">处理活跃路径为0</strong><strong nodeIndex="402">：</strong></p>
<p nodeIndex="120">如果活跃数等于0 ，判断no_path_retry是否设置（大于0为重试次数，小于0为其它策略），如果是，设置retry_tick（=mpp->no_path_retry * conf->checkint + 1）。然后退出检查。</p>
<p nodeIndex="121">（5） 新状态是up或ghost，通过dm_message主动通知DM，如果旧状态不是up或ghost，增加该multipath的活跃路径数，</p>
<p nodeIndex="122"><strong nodeIndex="403">处理增加前活跃为0</strong><strong nodeIndex="404">：</strong></p>
<p nodeIndex="123">如果增加前活跃路径数为0，并且no_path_retry>0，设置retry_tick=0，并且通过dm_message通知DM “queue_if_no_path”</p>
<p nodeIndex="124"><strong nodeIndex="405">处理failback</strong>：</p>
<p nodeIndex="125">根据mpp->pgfailback的设置（大于0为等待切换时间，小于0为其它策略），如果大于0，设置failback_tick=pgfailback+1；如果等于--FAILBACK_IMMEDIATE，通过need_switch_pathgroup重新刷新计算path的优先级，如果nextpg<>bestpg，通过dm_message通知DM bestpg（switchgroup）。</p>
<p nodeIndex="126"><strong nodeIndex="406">默认情况每条路径的优先级都是1(prio=const)</strong><strong nodeIndex="407">，因此need_switch_pathgroup</strong><strong nodeIndex="408">每次都会第一个Path</strong><strong nodeIndex="409">作为最优Path</strong><strong nodeIndex="410">。</strong></p>
<p nodeIndex="127">（6） 如果新状态是up，并且所在pathgroup的状态是disable，通过dm_message通知DM”enable”该pathgroup</p>
<p nodeIndex="128">（7） 如果新旧状态一致，并且一直是up或ghost状态，设置路径的checkint为原有值的两倍，最大为配置值的4倍。</p>
<p nodeIndex="129">（8） 刷新path的优先级，<strong nodeIndex="411">need_switch_pathgroup</strong>判断是否需要切换，如果需要：</p>
<p nodeIndex="130">如果pgfailback>0并且failback_tick<0，设置failback_tick=pgfailback+1；否则如果pgfailback等于--FAILBACK_IMMEDIATE，通过dm_message通知DM切换到bestpg。</p>
<p nodeIndex="131">Checkloop线程在检查完所有path后，会对vecs中所有的multipath做下列两件事情：</p>
<p nodeIndex="132">defered_failback_tick：</p>
<p nodeIndex="133">遍历所有multipath，对于设置了pgfailback的，<strong nodeIndex="412">failback_tick</strong><strong nodeIndex="413">减1</strong>，如果failback_tick等于0并且need_switch_pathgroup刷新路径返回1，<strong nodeIndex="414">switchgroup</strong>。</p>
<p nodeIndex="134">retry_count_tick：</p>
<p nodeIndex="135">遍历所有multipath，对于retry_tick>0，retry_tick减1，如果retry_tick等于0，通过dm_message通知DM “fail_if_no_path”</p>
<p nodeIndex="136">Switchgroup消息传递到内核，会修改内核multipath对象的current_pgpath=NULL和nextpg，failback消息传递到内核，会调用fail_path方法修改内核multipath对象的current_pgpath=NULL，之后的读写请求到multipath_target的map_io时就会选择的新的路径。</p>
<h3 nodeIndex="137">3.5.3 读写错误</h3>
<p nodeIndex="138">内核在对multipath_target进行读写结束后会调用do_end_io方法，如果读写发生错误时，该方法会调用fail_path，通过ioctl事件机制通知用户态</p>
<p nodeIndex="139">用户态multipathd收到事件后，调用update_multipath方法修改path的state为down，并且如果活跃路径数变为0，采用上述相同方法处理。</p>
<h2 nodeIndex="140">3.6 路径切换</h2>
<h3 nodeIndex="141">3.6.1 Failover</h3>
<p nodeIndex="142">在3.3.1路径检查中，发现路径故障，会通过dm_message消息通知内核DM，DM修改current_pgpath=NULL，在下次map_io时会选择新的路径。</p>
<p nodeIndex="143">选择方式为首先在当前优先组(currentpg)中查找，如果找不到，再从其它优先组中查找。</p>
<h3 nodeIndex="144">3.6.2 Failback</h3>
<p nodeIndex="145">在3.3.1路径检查中，如果pgfailback设置为IMMEDIATE，或者设定为等待时间failback_tick到了，用户态会通过dm_message消息通知内核DM，DM设置nextpg，修改current_pgpath=NULL, 在下次map_io时会选择新的路径。</p>
<p nodeIndex="146">选择方式为设定nextpg为当前优先组(currentpg)，直接在当前优先组中查找。</p>
<h3 nodeIndex="147">3.6.3 负载均衡</h3>
<p nodeIndex="148">内核multipath_target里，每个path有一个属性repeat_count（ round-robin模式下等于1000），每次map_io，repeat_count减1，等于0时，切换路径</p>
<p nodeIndex="149">选择方式为当前currentpg的path列表的头部取出一个path，然后再插入列表的尾部</p>
<h2 nodeIndex="150">3.7 路径全部失败</h2>
<p nodeIndex="151">在3.5中当选择路径失败时，即全部路径都出错了，这时map_io根据queue_if_no_path的值，决定是将请求排队（值为1），还是立即返回错误。通过测试发现如果请求在排队时，上层应用进程的状态是S(可中断睡眠)。</p>
<p nodeIndex="152">在3.4.1中用户态的no_path_retry配置（queue/fail）在向DM发送create multipath指令之后，会传递给DM。</p>
<p nodeIndex="153">在3.3.1路径检查中，如果no_path_retry设置为queue/fail，或者设定的等待时间retry_tick到了，用户态通过dm_message发送“queue_if_no_path”或“fail_if_no_path”消息给DM，DM由此设置queue_if_no_path。</p>
<h2 nodeIndex="154">3.8 IO排队</h2>
<h3 nodeIndex="155">3.8.1 排队触发</h3>
<p nodeIndex="156">用户态Multipath对象的hw_handle配置会传递到内核赋值内核multipath对象的hw_handle_name。</p>
<p nodeIndex="157">内核mapio在处理路径切换时，如果发现currentpg和要切换的pg是否相同，如果不相同会调用__switch_pg，__switch_pg方法除了设置currentpg外，会根据hw_handle_name的值设置queue_io和pg_init_required，如果hw_handle_name有值，都设置1。</p>
<p nodeIndex="158">Mapio处理完路径切换后，会判断currentpath!=NULL&&queue_io，或currentpath==NULL&&queue_if_no_path（3.7路径全部失败）满足一个条件后就将请求排队，将请求插入到queued_ios。</p>
<h3 nodeIndex="159">3.8.2 排队结束</h3>
<p nodeIndex="160">在一些可能要结束排队的触发事件发生后会通过queue_work调用process_queued_ios方法:</p>
<p nodeIndex="161">（1） 该方法首先判断currentpath是否为空，如果是，调用__choose_pgpath选择路径;</p>
<p nodeIndex="162">（2） 判断如果curentpath不等于空，queue_io是否等于0，或者等于空，queue_if_no_path是否等于0，这两个条件如果满足一个，设置must_queue=0；</p>
<p nodeIndex="163">（3） 然后判断pg_init_required是否设置，并且pg_init_in_progress没有设置，如果是调用__pg_init_all_paths初始化path；</p>
<p nodeIndex="164">（4） __pg_init_all_paths遍历每个路径，调用path的activate_path方法，设置pg_init_in_progress++，activate_path方法最终调用到Scsi_dh.c中的scsi_dh_activate方法（Scsi_device设备驱动需要实现），该方法在设备初始化完成后会调用pg_init_done；</p>
<p nodeIndex="165">（5） 如果must_queue==0，dispatch_queued_ios调用map_io处理排队的请求。</p>
<p nodeIndex="166">可能的触发条件有：</p>
<p nodeIndex="167">（1） queue_if_no_path方法在m->queue_if_no_path等于0（fail）时调用</p>
<p nodeIndex="168">（2） reinstate_path方法在m->nr_valid_paths原为0，情况下调用</p>
<p nodeIndex="169">（3） pg_init_done在path初始化完后调用</p>
<p nodeIndex="170">（4） map_io在处理完请求排队，在pg_init_required被设置（目的是处理path初始化），调用process_queued_ios。</p>
<h1 nodeIndex="171">4 主要函数实现说明</h1>
<h2 nodeIndex="172">4.1 Multipathd</h2>
<h3 nodeIndex="173">4.1.1 Checker</h3>
<p nodeIndex="174">Multipath采用动态加载的方式加载路径检查的方法；</p>
<p nodeIndex="175">directio主要是用aio的方法读取一个块的大小（最大4096）</p>
<p nodeIndex="176">readsector0是通过ioctl(sg_fd, SG_IO, &io_hdr)向scsi驱动器发送读指令。</p>
<h3 nodeIndex="177">4.1.2 Main(child)</h3>
<p nodeIndex="178">（1） load_config()</p>
<p nodeIndex="179">（2） init_checker()</p>
<p nodeIndex="180">（3） init_prio(): 默认为const,每条路径的优先级都是1</p>
<p nodeIndex="181">（4） configure()</p>
<p nodeIndex="182">（5） 创建几个线程（checkerloop、ueventloop、uxlsnrloop(注册各种CLI请求（Multipathd -k）处理方法)）</p>
<h3 nodeIndex="183">4.1.3 Configure</h3>
<p nodeIndex="184">（1）path_discovery</p>
<p nodeIndex="185">（2）filter_path：根据blacklist过滤路径</p>
<p nodeIndex="186">（3）map_discovery：dm_get_maps调用libdevicemap的接口、setup_multipath（update_multipath_strings）</p>
<p nodeIndex="187">（4）coalesce_paths</p>
<p nodeIndex="188">（5）coalesce_maps</p>
<p nodeIndex="189">（6）sync_maps_state: 遍历所有multipath的所有path,根据dmstate和state的差别，调用dm_reinstate_path或dm_fail_path向DM发送对应message。</p>
<p nodeIndex="190">（7）遍历每个multipath,调用start_waiter_thread，启动一个dm事件监听线程，update_multipath处理产生的事件，首先查找Multipath，然后setup_multipath，如果查找失败或setup失败，退出线程。之后判断所有path的dmstate如果是failed，修改path的state为down,并且如果path的原有state是up(ghost)，调用update_queue_mode_del_path</p>
<h3 nodeIndex="191">4.1.4 path_discovery</h3>
<p nodeIndex="192">(1)从“/sys/block”目录下获取设备名（char*）</p>
<p nodeIndex="193">(2)根据设备名在path_vec中查找，不存在添加</p>
<p nodeIndex="194">（3）path_info获取路径相关信息（state\priority\uid\scsi设备的serialno）</p>
<p nodeIndex="195">discovery.c(.h)里其它方法都是由path_info调用，向各种设备获取各种设备信息和状态</p>
<h3 nodeIndex="196">4.1.5 dm_get_maps</h3>
<p nodeIndex="197">（1）dm_task_create(DM_DEVICE_LIST)获取所有DM设备名称names</p>
<p nodeIndex="198">（2）遍历names</p>
<p nodeIndex="199">（3）dm_type(),通过name判断是否是multipath设备(通过获取DM_DEVICE_TABLE)</p>
<p nodeIndex="200">（4）如果是multipath设备，获取mpp->alias,mpp->size,mpp->params,mpp->status,mpp->wwid,mpp->dmi(dm_info),插入全局的mpvec</p>
<p nodeIndex="201">（5）至（2）完成遍历</p>
<h3 nodeIndex="202">4.1.6 setup_multipath</h3>
<p nodeIndex="203">（1）dm_get_info （maybe 重复了，而且可能有内存泄露！！！）</p>
<p nodeIndex="204">（2）dm_map_present判断dm_info.exists</p>
<p nodeIndex="205">（3）set_multipath_wwid （在map_discovery的调用里，不会再执行）</p>
<p nodeIndex="206">（4）通过wwid,find_mpe赋值mpp->mpe</p>
<p nodeIndex="207">（5）update_multipath_strings，如果返回1，dm_get_name获取new_alias，如果成功，说明发生rename,赋值当前mpp->alias，goto out(remove_map(mpp, vecs, PURGE_VEC))</p>
<p nodeIndex="208">（6）extract_hwe_from_path: 从Multipath中选择PGSTATE_ACTIVE和PGSTATE_ENABLED的pgp,再选择非PSTATE_FAILED状态的path,使用find_hwe，通过path->vendor_id,product_id,rev查询conf->hwtable,并赋值path->hwe,然后返回hwe</p>
<p nodeIndex="209">（7）通过mpp->mpe或mpp->hwe或conf，设置Multipath各种属性（rr_weight\pgfailback\no_path_retry\pg_timeout\flush_on_last_del）</p>
<h3 nodeIndex="210">4.1.7 update_multipath_strings</h3>
<p nodeIndex="211">（1）free_multipath_attributes,free_pgvec</p>
<p nodeIndex="212">（2）update_multipath_table</p>
<p nodeIndex="213">（3）update_multipath_status</p>
<h3 nodeIndex="214">4.1.8 update_multipath_table</h3>
<p nodeIndex="215">（1）dm_get_map：获取mpp->params</p>
<p nodeIndex="216">（2）disassemble_map：从params中解析出multipath的配置</p>
<h3 nodeIndex="217">4.1.9 disassemble_map</h3>
<p nodeIndex="218">（1）从mpp->params解析出mpp->no_path_retry,mpp->hwhandler</p>
<p nodeIndex="219">（2）解析出path group的个数</p>
<p nodeIndex="220">（3）（0，num_pg）循环</p>
<p nodeIndex="221">（4）如果mpp->selector没有赋值，先赋值</p>
<p nodeIndex="222">（5）alloc_pathgroup()</p>
<p nodeIndex="223">（6）解析出pgp下面的path个数</p>
<p nodeIndex="224">（7）（0，num_paths）循环</p>
<p nodeIndex="225">（8）解析出path的设备名，从全局pathvec查找path，没有则alloc_path()，插入pathvec</p>
<p nodeIndex="226">（9）将path插入到pgp->paths</p>
<p nodeIndex="227">（10）如果mpp->wwid没有赋值，用path->wwid赋值,否则如果path->wwid没有赋值，用mpp->wwid赋值</p>
<p nodeIndex="228">（11）解析出mpp->minio（？）</p>
<p nodeIndex="229">（12）至（7），至（3）</p>
<h3 nodeIndex="230">4.1.10 update_multipath_status</h3>
<p nodeIndex="231">（1）dm_get_status，通过dm_get_status获取设备状态，赋值mpp->status（char*）</p>
<p nodeIndex="232">（2）disassemble_status:解析mpp->status</p>
<p nodeIndex="233">赋值mpp->queueio</p>
<p nodeIndex="234">循环赋值所以pgp,赋值status(PGSTATE_DISABLED\PGSTATE_ACTIVE\PGSTATE_ENABLED\PGSTATE_UNDEF)</p>
<p nodeIndex="235">循环赋值pgp下面所有的path的状态：PSTATE_FAILED、PSTATE_ACTIVE，和failcount</p>
<p nodeIndex="236">解析出mpp->minio</p>
<h3 nodeIndex="237">4.1.11 coalesce_paths</h3>
<p nodeIndex="238">（1）遍历所有的路径Path</p>
<p nodeIndex="239">（2）针对mpp=NULL的Path(pp1)，调用add_map_with_path方法，创建multipath(mpp),赋值Path的mpp, adopt_paths</p>
<p nodeIndex="240">（3）从当前迭代位置K往后遍历Path,当Path（pp2）的wwid和当前Path的wwid相等时，如果pp2的size不等于mpp的size,或pp2的priority为undef，设置mpp的action为ACT_REJECT</p>
<p nodeIndex="241">（4）verify_paths: 遍历Multipath的path,如果path->sysdev=null或者从sysfs_get_dev获取不到设备信息，从multipath中删除path,同时从全局的pathvec中删除path.</p>
<p nodeIndex="242">（5）setup_map: 通过mpp->mpe或mpp->hwe或conf，设置Multipath各种属性（pgfailback\pgpolicy\selector等，mpp->pgpolicyfn(mpp)：根据pgpolicy从path生成pathgroup）,调用assemble_map，将mulitpath相关配置编码到mp->params.</p>
<p nodeIndex="243">（6）select_action: 如果mpp->action==undef，通过alias、wwid在全局的mpvec是否存在，设置action为create,rename,reload，nothing等。</p>
<p nodeIndex="244">（7）domap：根据action具体类别，调用相应dm接口处理</p>
<p nodeIndex="245">（8）通过dm_message设置no_path_retry和pg_timeout</p>
<p nodeIndex="246">（9）如果mpp的action不是reject，将mpp加入newmp(与mpvec同类型)</p>
<p nodeIndex="247">（10）转向（1）直至遍历全部path</p>
<p nodeIndex="248">（11）遍历newmpp中所有Multipath,如果Multipath是deadmap(所有path->dev=null,not in sysfs)，从newmp和全局mpvec中删除Multipath，调用dm_flush_map通知DM删除该mpp。</p>
<h3 nodeIndex="249">4.1.12 adopt_paths</h3>
<p nodeIndex="250">（1）update_mpp_paths，使用multipath->pg->paths更新multipath->paths</p>
<p nodeIndex="251">（2）遍历全局的pathvec, 如果Path的wwid等于multipath的wwid，将Path加入multipath->paths,同时path_info(path).</p>
<h3 nodeIndex="252">4.1.13 coalesce_maps</h3>
<p nodeIndex="253">将vecs->mpvec中已经不存在于newmp(coalesce_paths方法生成)的mpp,调用dm_flush_map从DM内核删除，如果删除失败，将mpp插入newmp,setup_multipath(),从oldmp删除</p>
<h3 nodeIndex="254">4.1.14 find_hwe</h3>
<p nodeIndex="255">Hwtable.c记录了默认支持的硬件驱动器，struct hwentry default_hw[]</p>
<p nodeIndex="256">conf->hwtable，首先是来自上述的default_hw，其次是由load_config的init_data()从配置文件的devices项读取.</p>
<p nodeIndex="257">find_hwe 根据vendor和product，从conf->hwtable中查找hwentry</p>
<h3 nodeIndex="258">4.1.15 find_mpe</h3>
<p nodeIndex="259">conf->mptable,由load_config的init_data()从配置文件的multipaths项读取。</p>
<p nodeIndex="260">find_mpe从conf->mptable中根据wwid查找mpentry</p>
<h3 nodeIndex="261">4.1.16 checkerloop</h3>
<p nodeIndex="262">（1）check_path：get_state获取Path状态，根据最新状态调用update_multipath_strings更新Multipath状态，或主动更新内核DM的状态。</p>
<p nodeIndex="263">（2）defered_failback_tick: Multipath的failback_tick每次减1，当等于0时，根据是否需要（mpp->pgfailback == -FAILBACK_MANUAL）切换调用switchgroup</p>
<p nodeIndex="264">（3）retry_count_tick: Multipath的retry_tick每次减1，当等于0时，调用dm_queue_if_no_path</p>
<h3 nodeIndex="265">4.1.17 get_state</h3>
<p nodeIndex="266">（1）首先用path_offline通过sysfs获取设备状态(sysfs_get_state)</p>
<p nodeIndex="267">（2）如果是up状态，再调用checker_check使用具体的检查路径方法进行check</p>
<h3 nodeIndex="268">4.1.18 check_path</h3>
<p nodeIndex="269">（1）newstate=get_state(pp)</p>
<p nodeIndex="270">（2）如果状态是PATH_WILD（sysfs相关操作失败）或PATH_UNCHECKED(获取或设置checker失败)，pathinfo(pp, conf->hwtable, 0)，退出</p>
<p nodeIndex="271">（3）如果状态是PATH_PENDING，pp->tick=1</p>
<p nodeIndex="272">（4）如果新状态和旧状态不相等，调用（5）-（8）</p>
<p nodeIndex="273">（5）调用update_multipath_strings()更新pp->mpp，如果返回大于0，或者新状态是PATH_DOWN或者PATH_SHAKY，并且oldstate是PATH_UP或PATH_GHOST，调用fail_path（调用dm_fail_path通知DM，update_queue_mode_del_path修改retry_tick），pp->mpp->failback_tick = 0，退出</p>
<p nodeIndex="274">（6）（新状态等于PATH_UP或PATH_GHOST），如果oldstate不等于PATH_UP或者不等于PATH_GHOST，reinstate_path（pp,1）否则reinstate_path(pp,0)（dm_reinstate_path通知DM，update_queue_mode_add_path）</p>
<p nodeIndex="275">（7）如果pp->mpp->pgfailback == FAILBACK_IMMEDIATE并且need_switch_pathgroup(pp>mpp, 1)，调用switch_pathgroup(pp->mpp)</p>
<p nodeIndex="276">（8）如果新状态是PATH_UP，调用enable_group（dm_enablegroup通知DM）</p>
<p nodeIndex="277">（9）如果新旧状态相等，并且newstate是PATH_UP或PATH_GHOST，pp->checkint自增1</p>
<p nodeIndex="278">（10）pathinfo（DI_PRIO），refreshing path prio</p>
<p nodeIndex="279">（11）处理need_switch_pathgroup，根据pgfailback可能switch_pathgroup()</p>
<h3 nodeIndex="280">4.1.19 update_queue_mode_del_path</h3>
<p nodeIndex="281">（1）mpp活跃路径数nr_active减1</p>
<p nodeIndex="282">（2）如果mpp的nr_active等于0并且no_path_retry大于0，stat_queueing_timeouts自增1，设置retry_tick等于mpp->no_path_retry * conf->checkint + 1。</p>
<h3 nodeIndex="283">4.1.20 update_queue_mode_add_path</h3>
<p nodeIndex="284">（1）增加活跃路径数nr_active</p>
<p nodeIndex="285">（2）如果增加前活跃路径数为0，并且no_path_retry>0,设置retry_tick=0，通知DM “queue_if_no_path”</p>
<h3 nodeIndex="286">4.1.21 need_switch_pathgroup</h3>
<p nodeIndex="287">（1）如果mpp->pgfailback=-FAILBACK_MANUAL，返回0</p>
<p nodeIndex="288">（2）如果参数refresh设置为1，调用pathinfo刷新path的prio</p>
<p nodeIndex="289">（3）select_path_group计算mpp的最优pathgroup，先比较平均优先级，再比较enable路径个数。</p>
<p nodeIndex="290">（4）如果bestpg不等于nextpg，返回1</p>
<p nodeIndex="291">（5）返回0</p>
<h2 nodeIndex="292">4.2 libdevmapper</h2>
<p nodeIndex="293">libdevmapper提供大部分接口是通过ioctl方法向“/dev/mapper/control”设备(设备号等于/proc/misc中dev-mapper驱动的设备号)通信完成的。</p>
<p nodeIndex="294">支持的请求类型定义在：_cmd_data_v4</p>
<h2 nodeIndex="295">4.3 Multipath_target</h2>
<p nodeIndex="296">multipath_target的主要方法：</p>
<p nodeIndex="297">multipath_ctr</p>
<p nodeIndex="298">multipath_dtr</p>
<p nodeIndex="299">multipath_map:调用map_io</p>
<p nodeIndex="300">multipath_status</p>
<p nodeIndex="301">multipath_message: 根据具体的消息类型会调用fail_path, queue_if_no_path, switch_pg_num等方法</p>
<p nodeIndex="302">multipath_end_io</p>
<h3 nodeIndex="303">4.3.1 DM_ioctl.c</h3>
<p nodeIndex="304">处理来自libdevmapper对内核信息的各种请求，主要的方法有：</p>
<p nodeIndex="305">list_devices</p>
<p nodeIndex="306">dev_create</p>
<p nodeIndex="307">table_status (会调用到target的相应方法，即multipath_status)</p>
<p nodeIndex="308">target_message(最终调用target的相应方法，即multipath_message)</p>
<h3 nodeIndex="309">4.3.2 map_io</h3>
<p nodeIndex="310">（1）if (!m->current_pgpath ||</p>
<p nodeIndex="311">(!m->queue_io && (m->repeat_count && -m>repeat_count == 0)))</p>
<p nodeIndex="312">__choose_pgpath(m, nr_bytes);</p>
<p nodeIndex="313">路径失败、路径恢复、负载均衡（repeat_count）等原因，会触发调用__choose_pgpath</p>
<p nodeIndex="314">（2）bdev = pgpath->path.dev->bdev;</p>
<p nodeIndex="315">clone->q = bdev_get_queue(bdev);</p>
<p nodeIndex="316">clone->rq_disk = bdev->bd_disk;</p>
<p nodeIndex="317">return DM_MAPIO_REMAPPED;</p>
<h3 nodeIndex="318">4.3.3 __choose_pgpath</h3>
<p nodeIndex="319">（1）首先根据m->next_pg选择，其次在current_pg中寻找，最后再遍历m->priority_groups，调用__choose_path_in_pg()寻找path.</p>
<p nodeIndex="320">（2）_choose_path_in_pg()：使用path_selector_type中调用的select_path的方法选择path,然后设置m->current_pgpath，如果m->current_pg和选择pg不相同，调用_switch_pg。</p>
<p nodeIndex="321">round-robin：rr_select_path:从valid_paths列表的头部取出路径，然后插入到尾部，返回该路径。用path的repeat_count赋值multipath的repeat_count。</p>
<p nodeIndex="322">（3）_switch_pg首先设置m->currentpg，然后根据hw_handler_name是否赋值，如果是，设置pg_init_required=1，queue_io=1，否则都设置为0。</p>
<h3 nodeIndex="323">4.3.4 fail_path</h3>
<p nodeIndex="324">（1）调用Path所在优先组pg的path_selector_type中定义的fail_path方法</p>
<p nodeIndex="325">round-robin:将path加入selector的invalid_paths列表中</p>
<p nodeIndex="326">（2）pgpath->is_active = 0;m->nr_valid_paths-;如果m>current_pgpath等于该path，设置m->current_pgpath=NULL</p>
<p nodeIndex="327">（3）dm_path_ueventt产生一个fail_path的事件通过netlink events机制通知用户态</p>
<p nodeIndex="328">（4）schedule_work(&m->trigger_event)触发一个event以唤起用户态的对该Multipath事件的监听线程</p>
<h3 nodeIndex="329">4.3.5 queue_if_no_path</h3>
<p nodeIndex="330">根据传递的参数queue_if_no_path设置multipath的queue_if_no_path。</p>
<h3 nodeIndex="331">4.3.6 switch_pg_num</h3>
<p nodeIndex="332">对用户态的switch_pg的指令消息进行处理，修改multipath的next_pg为相应组序号（参数）的优先组，设置m->current_pgpath = NULL;m->current_pg = NULL。</p>
<p nodeIndex="333">schedule_work(&m->trigger_event)触发一个event以唤起用户态的对该Multipath事件的监听线程</p>
<h1 nodeIndex="334">5 遗留问题</h1>
<p nodeIndex="335">（1） 内核devmapper建立过程分为两个步骤，创建mapped_device和table_load(创建table和target)，用户态触发内核创建的代码应该在coalesce_paths中，但是这个方法如何实现触发这两个过程的？</p>
</div>