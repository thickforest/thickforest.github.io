---
layout: post
title: sysfs-procfs-守候心田-ChinaUnix博客
categories:
- Pocket
tags:
---
原文地址：http://blog.chinaunix.net/uid-27411029-id-3522299.html

收藏时间：2015-01-07 16:19:09

<div  >
<p nodeIndex="256">sysfs</p>
是 Linux 内核中设计较新的一种虚拟的基于内存的文件系统，它的作用与 proc 有些类似，但除了与 proc<br nodeIndex="342">
相同的具有查看和设定内核参数功能之外，还有为 Linux 统一设备模型作为管理之用。相比于 proc 文件系统，使用 sysfs<br nodeIndex="343">
导出内核数据的方式更为统一，并且组织的方式更好，它的设计从 proc 中吸取了很多教训。本文就 sysfs 的挂载点 /sys 目录结构、其与<br nodeIndex="344">
Linux 统一设备模型的关系、常见属性文件的用法等方面对 sysfs 作入门介绍，并且就内核编程方面，以具体的例子来展示如何添加 sysfs<br nodeIndex="345">
支持。<br nodeIndex="349"><strong nodeIndex="350">sysfs 的历史，其与 proc 的关系？</strong><br nodeIndex="351">
sysfs 本身并不是一项很新的技术，但笔者发现，虽然 sysfs 从2003年诞生至今已有5年，但人们对 sysfs<br nodeIndex="352">
依然缺乏了解；一个很重要的原因可能是缺乏文档， Linux 内核方面最重要的理论书籍“Linux 设备驱动第3版”和“理解 Linux<br nodeIndex="353">
内核第2版”都诞生于2003年前后，并且从那以后尚未有再版过，其它一些重要文章则多对 sysfs 与 proc 相提并论且举例常常只有<br nodeIndex="354">
proc，这导致了 sysfs 的很多重要概念至今仍鲜为人知，因此有必要对 sysfs 作更多介绍，这是写作本文的初衷。
sysfs 与 /sys
sysfs<br nodeIndex="359">
文件系统总是被挂载在 /sys 挂载点上。虽然在较早期的2.6内核系统上并没有规定 sysfs 的标准挂载位置，可以把 sysfs<br nodeIndex="360">
挂载在任何位置，但较近的2.6内核修正了这一规则，要求 sysfs 总是挂载在 /sys 目录上；针对以前的 sysfs<br nodeIndex="361">
挂载位置不固定或没有标准被挂载，有些程序从 /proc/mounts 中解析出 sysfs<br nodeIndex="362">
是否被挂载以及具体的挂载点，这个步骤现在已经不需要了。请参考附录给出的<br nodeIndex="363"><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#sysfs-rules.txt" target="_blank" nodeIndex="364">sysfs-rules.txt</a><br nodeIndex="365">
文件链接。
sysfs 与 proc
sysfs 与 proc 相比有很多优点，最重要的莫过于设计上的清晰。一个 proc 虚拟文件可能有内部格式，如 /proc/scsi/scsi<br nodeIndex="370">
，它是可读可写的，(其文件权限被错误地标记为了 0444<br nodeIndex="371">
！，这是内核的一个BUG)，并且读写格式不一样，代表不同的操作，应用程序中读到了这个文件的内容一般还需要进行字符串解析，而在写入时需要先用字符串<br nodeIndex="372">
格式化按指定的格式写入字符串进行操作；相比而言， sysfs 的设计原则是一个属性文件只做一件事情， sysfs<br nodeIndex="373">
属性文件一般只有一个值，直接读取或写入。整个 /proc/scsi 目录在2.6内核中已被标记为过时(LEGACY)，它的功能已经被相应的 /sys 属性文件所完全取代。新设计的内核机制应该尽量使用 sysfs 机制，而将 proc 保留给纯净的“进程文件系统”。<a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#main" target="_blank" nodeIndex="384"><strong nodeIndex="385">回页首</strong></a><br nodeIndex="386">
初识 /sys<strong nodeIndex="341">清单 1. 与 /sys 文件系统的一次交互<em nodeIndex="389">(视内核版本号和外接设备的不同，在您的系统上执行这些命令的结果可能与此有所不同)</em></strong><br nodeIndex="390">
$ ls -F /sys<br nodeIndex="391">
block/ bus/ class/ dev/ devices/ firmware/ fs/ kernel/ module/ power/<br nodeIndex="392">
$ ls -F /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/<br nodeIndex="393">
broken_parity_status enable modalias resource0 rom uevent<br nodeIndex="394">
class irq msi_bus resource0_wc subsystem@ vendor<br nodeIndex="395">
config local_cpulist power/ resource1 subsystem_device<br nodeIndex="396">
device local_cpus resource resource2 subsystem_vendor
这<br nodeIndex="399">
是在 Fedora 10 的 2.6.27.5-117.fc10.i686 的内核上，可以看到在 /sys 目录下有 block, bus,<br nodeIndex="400">
class, dev, devices, firmware, fs, kernel, module, power<br nodeIndex="401">
这些子目录，本文将分别介绍这些目录存在的含义。
第二个 ls 命令展示了在一个 pci 设备目录下的文件， "ls"<br nodeIndex="404">
命令的 "-F" 命令为所列出的每个文件使用后缀来显示文件的类型，后缀 "/" 表示列出的是目录，后缀 "@"<br nodeIndex="405">
表示列出的是符号链接文件。可以看到第二个目录下包含有普通文件 (regular file) 和符号链接文件 (symbolic link<br nodeIndex="406">
file) ，本文也将以这个具体的设备为例说明其中每一个普通文件的用途。<a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#main" target="_blank" nodeIndex="417"><strong nodeIndex="418">回页首</strong></a><br nodeIndex="419">
/sys 文件系统下的目录结构
/sys 下的目录结构是经过精心设计的：在 /sys/devices<br nodeIndex="422">
下是所有设备的真实对象，包括如视频卡和以太网卡等真实的设备，也包括 ACPI 等不那么显而易见的真实设备、还有 tty, bonding<br nodeIndex="423">
等纯粹虚拟的设备；在其它目录如 class, bus 等中则在分类的目录中含有大量对 devices 中真实对象引用的符号链接文件；<br nodeIndex="424"><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#listing1" target="_blank" nodeIndex="425">清单1</a><br nodeIndex="426">
中在 /sys 根目录下顶层目录的意义如下：<strong nodeIndex="429">表 1. /sys 下的目录结构</strong><br nodeIndex="430">
/sys 下的子目录所包含的内容/sys/devices这是内核对系统中所有设备的分层次表达模型，也是 /sys 文件系统管理设备的最重要的目录结构，下文会对它的内部结构作进一步分析；/sys/dev这个目录下维护一个按字符设备和块设备的主次号码 (major:minor)链接到真实的设备(/sys/devices下)的符号链接文件，它是在内核 2.6.26 首次引入；/sys/bus这是内核设备按总线类型分层放置的目录结构， devices 中的所有设备都是连接于某种总线之下，在这里的每一种具体总线之下可以找到每一个具体设备的符号链接，它也是构成 Linux 统一设备模型的一部分；/sys/class这是按照设备功能分类的设备模型，如系统所有输入设备都会出现在 /sys/class/input 之下，而不论它们是以何种总线连接到系统。它也是构成 Linux 统一设备模型的一部分；/sys/block这<br nodeIndex="431">
里是系统中当前所有的块设备所在，按照功能来说放置在 /sys/class 之下会更合适，但只是由于历史遗留因素而一直存在于<br nodeIndex="432">
/sys/block, 但从 2.6.22 开始就已标记为过时，只有在打开了 CONFIG_SYSFS_DEPRECATED<br nodeIndex="433">
配置下编译才会有这个目录的存在，并且在 2.6.26 内核中已正式移到 /sys/class/block, 旧的接口 /sys/block<br nodeIndex="434">
为了向后兼容保留存在，但其中的内容已经变为指向它们在 /sys/devices/ 中真实设备的符号链接文件；/sys/firmware这里是系统加载固件机制的对用户空间的接口，关于固件有专用于固件加载的一套API，在附录 LDD3 一书中有关于内核支持固件加载机制的更详细的介绍；/sys/fs这<br nodeIndex="435">
里按照设计是用于描述系统中所有文件系统，包括文件系统本身和按文件系统分类存放的已挂载点，但目前只有 fuse,gfs2 等少数文件系统支持<br nodeIndex="436">
sysfs 接口，一些传统的虚拟文件系统(VFS)层次控制参数仍然在 sysctl (/proc/sys/fs) 接口中中；/sys/kernel这里是内核所有可调整参数的位置，目前只有 uevent_helper, kexec_loaded, mm, 和新式的 slab 分配器等几项较新的设计在使用它，其它内核可调整参数仍然位于 sysctl (/proc/sys/kernel) 接口中 ;/sys/module这里有系统中所有模块的信息，不论这些模块是以内联(inlined)方式编译到内核映像文件(vmlinuz)中还是编译为外 部模块(ko文件)，都可能会出现在 /sys/module 中：
<ul nodeIndex="258"><li nodeIndex="257">编译为外部模块(ko文件)在加载后会出现对应的 /sys/module//, 并且在这个目录下会出现一些属性文件和属性目录来表示此外部模块的一些信息，如版本号、加载状态、所提供的驱动程序等；</li>
<li nodeIndex="259">编译为内联方式的模块则只在当它有非0属性的模块参数时会出现对应的 /sys/module/, 这些模块的可用参数会出现在 /sys/modules//parameters/ 中，
<ul nodeIndex="261"><li nodeIndex="260">如 /sys/module/printk/parameters/time 这个可读写参数控制着内联模块 printk 在打印内核消息时是否加上时间前缀；</li>
<li nodeIndex="262">所有内联模块的参数也可以由<br nodeIndex="437">
".="<br nodeIndex="438">
的形式写在内核启动参数上，如启动内核时加上参数 "printk.time=1" 与 向<br nodeIndex="439">
"/sys/module/printk/parameters/time" 写入1的效果相同；</li>
</ul></li>
<li nodeIndex="263">没有非0属性参数的内联模块不会出现于此。</li>
</ul>
/sys/power这里是系统中电源选项，这个目录下有几个属性文件可以用于控制整个机器的电源状态，如可以向其中写入控制命令让机器关机、重启等。 /sys/slab (对应 2.6.23 内核，在 2.6.24 以后移至 /sys/kernel/slab)从2.6.23<br nodeIndex="440">
开始可以选择 SLAB 内存分配器的实现，并且新的 SLUB（Unqueued Slab<br nodeIndex="441">
Allocator）被设置为缺省值；如果编译了此选项，在 /sys 下就会出现 /sys/slab ，里面有每一个 kmem_cache<br nodeIndex="442">
结构体的可调整参数。对应于旧的 SLAB 内存分配器下的 /proc/slabinfo 动态调整接口，新式的<br nodeIndex="443">
/sys/kernel/slab/ 接口中的各项信息和可调整项显得更为清晰。<br nodeIndex="446">
接下来对 /sys/devices/ 下的目录结构作进一步探讨：<strong nodeIndex="449">清单 2. 查看 /sys/devices/ 的目录结构</strong><br nodeIndex="450">
$ ls -F /sys/devices/<br nodeIndex="451">
isa/ LNXSYSTM:00/ pci0000:00/ platform/ pnp0/ pnp1/ system/ virtual/
可以看到，在 /sys/devices/ 目录下是按照设备的基本总线类型分类的目录，再进入进去查看其中的 PCI 类型的设备：<strong nodeIndex="456">清单 3. 查看 /sys/devices/pci0000:00/ 的目录结构</strong><br nodeIndex="457">
$ ls -F /sys/devices/pci0000:00/<br nodeIndex="458">
0000:00:00.0/ 0000:00:02.5/ 0000:00:03.1/ 0000:00:0e.0/ power/<br nodeIndex="459">
0000:00:01.0/ 0000:00:02.7/ 0000:00:03.2/ firmware_node@ uevent<br nodeIndex="460">
0000:00:02.0/ 0000:00:03.0/ 0000:00:03.3/ pci_bus/
在 /sys/devices/pci0000:00/ 目录下是按照 PCI 总线接入的设备号分类存放的目录，再查看其中一个，<strong nodeIndex="465">清单 4. 查看 /sys/devices/pci0000:00/ 的目录结构</strong><br nodeIndex="466">
$ ls -F /sys/devices/pci0000:00/0000:00:01.0/<br nodeIndex="467">
0000:01:00.0/ device local_cpus power/ subsystem_vendor<br nodeIndex="468">
broken_parity_status enable modalias resource uevent<br nodeIndex="469">
class irq msi_bus subsystem@ vendor<br nodeIndex="470">
config local_cpulist pci_bus/ subsystem_device
可以看到，其中有一个目录 0000:01:00.0/, 其它都是属性文件和属性组，而如果对 0000:01:00.0/ 子目录中进行再列表查看则会得到<br nodeIndex="473"><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#listing1" target="_blank" nodeIndex="474">清单1</a><br nodeIndex="475">
的目录结构。
继续以上过程可以了解整个目录树的结构，这里把它整理成<br nodeIndex="478"><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#figure1" target="_blank" nodeIndex="479">图 1. sysfs 目录层次图</a><br nodeIndex="482"><strong nodeIndex="483">图 1. sysfs 目录层次图</strong><div id="RIL_IMG_1" class="RIL_IMG"><img src="/media/posts_images/2015-01-07-809193403/1"/></div><br nodeIndex="488">
其中涉及到 ksets, kobjects, attrs 等很多术语，这就不得不提到 Linux 统一设备模型。<a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#main" target="_blank" nodeIndex="499"><strong nodeIndex="500">回页首</strong></a><br nodeIndex="501">
Linux 统一设备模型
在<br nodeIndex="504">
Linux 2.5<br nodeIndex="505">
内核的开发过程中，人们设计了一套新的设备模型，目的是为了对计算机上的所有设备进行统一地表示和操作，包括设备本身和设备之间的连接关系。这个模型是在<br nodeIndex="506">
分析了 PCI 和 USB<br nodeIndex="507">
的总线驱动过程中得到的，这两个总线类型能代表当前系统中的大多数设备类型，它们都有完善的热挺拔机制和电源管理的支持，也都有级连机制的支持，以桥接的<br nodeIndex="508">
PCI/USB 总线控制器的方式可以支持更多的 PCI/USB<br nodeIndex="509">
设备。为了给所有设备添加统一的电源管理的支持，而不是让每个设备中去独立实现电源管理的支持，人们考虑的是如何尽可能地重用代码；而且在有层次模型的<br nodeIndex="510">
PCI/USB 总线中，必须以合理形式展示出这个层次关系，这也是电源管理等所要求的必须有层次结构。
如在一个典型的<br nodeIndex="513">
PC 系统中，中央处理器(CPU)能直接控制的是 PCI 总线设备，而 USB 总线设备是以一个 PCI 设备(PCI-USB桥)的形式接入在<br nodeIndex="514">
PCI 总线设备上，外部 USB 设备再接入在 USB 总线设备上；当计算机执行挂起(suspend)操作时， Linux 内核应该以<br nodeIndex="515">
“外部USB设备->USB总线设备->PCI总线设备”<br nodeIndex="516">
的顺序通知每一个设备将电源挂起；执行恢复(resume)时则以相反的顺序通知；反之如果不按此顺序则将有设备得不到正确的电源状态变迁的通知，将无法<br nodeIndex="517">
正常工作。
sysfs 是在这个 Linux 统一设备模型的开发过程中的一项副产品(见<br nodeIndex="520"><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#resources" target="_blank" nodeIndex="521">参考资料</a><br nodeIndex="522">
中 Greg K. Hartman 写作的 LinuxJournal<br nodeIndex="523">
文章)。为了将这些有层次结构的设备以用户程序可见的方式表达出来，人们很自然想到了利用文件系统的目录树结构（这是以 UNIX<br nodeIndex="524">
方式思考问题的基础，一切都是文件！）在这个模型中，有几种基本类型，它们的对应关系见<br nodeIndex="525"><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#table2" target="_blank" nodeIndex="526">表 2. Linux 统一设备模型的基本结构</a><br nodeIndex="527">
：<strong nodeIndex="530">表 2. Linux 统一设备模型的基本结构</strong><br nodeIndex="531">
类型所包含的内容对应内核数据结构对应/sys项设备(Devices)设备是此模型中最基本的类型，以设备本身的连接按层次组织struct device/sys/devices/*/*/.../设备驱动(Device Drivers)在一个系统中安装多个相同设备，只需要一份驱动程序的支持struct device_driver/sys/bus/pci/drivers/*/总线类型(Bus Types)在整个总线级别对此总线上连接的所有设备进行管理struct bus_type/sys/bus/*/设备类别(Device Classes)这是按照功能进行分类组织的设备层次树；如 USB 接口和 PS/2 接口的鼠标都是输入设备，都会出现在 /sys/class/input/ 下struct class/sys/class/*/
从内核在实现它们时所使用的数据结构来说， Linux 统一设备模型又是以两种基本数据结构进行树型和链表型结构组织的：<br nodeIndex="534"><ul nodeIndex="265"><li nodeIndex="264">kobject: 在 Linux 设备模型中最基本的对象，它的功能是提供引用计数和维持父子(parent)结构、平级(sibling)目录关系，上面的 device, device_driver 等各对象都是以 kobject 基础功能之上实现的；<br nodeIndex="535">
struct kobject {<br nodeIndex="536">
const char *name;<br nodeIndex="537">
struct list_head entry;<br nodeIndex="538">
struct kobject *parent;<br nodeIndex="539">
struct kset *kset;<br nodeIndex="540">
struct kobj_type *ktype;<br nodeIndex="541">
struct sysfs_dirent *sd;<br nodeIndex="542">
struct kref kref;<br nodeIndex="543">
unsigned int state_initialized:1;<br nodeIndex="544">
unsigned int state_in_sysfs:1;<br nodeIndex="545">
unsigned int state_add_uevent_sent:1;<br nodeIndex="546">
unsigned int state_remove_uevent_sent:1;<br nodeIndex="547">
};<br nodeIndex="548">
其中 struct kref 内含一个 atomic_t 类型用于引用计数， parent 是单个指向父节点的指针， entry 用于父 kset 以链表头结构将 kobject 结构维护成双向链表；<br nodeIndex="549"></li>
<li nodeIndex="266">kset: 它用来对同类型对象提供一个包装集合，在内核数据结构上它也是由内嵌一个 kboject 实现，因而它同时也是一个 kobject (面向对象 OOP 概念中的继承关系) ，具有 kobject 的全部功能；<br nodeIndex="550">
struct kset {<br nodeIndex="551">
struct list_head list;<br nodeIndex="552">
spinlock_t list_lock;<br nodeIndex="553">
struct kobject kobj;<br nodeIndex="554">
struct kset_uevent_ops *uevent_ops;<br nodeIndex="555">
};<br nodeIndex="556">
其中的 struct list_head list 用于将集合中的 kobject 按 struct list_head entry 维护成双向链表；<br nodeIndex="557"></li>
</ul>
涉及到文件系统实现来说， sysfs 是一种基于 ramfs<br nodeIndex="560">
实现的内存文件系统，与其它同样以 ramfs 实现的内存文件系统(configfs,debugfs,tmpfs,...)类似， sysfs<br nodeIndex="561">
也是直接以 VFS 中的 struct inode 和 struct dentry 等 VFS<br nodeIndex="562">
层次的结构体直接实现文件系统中的各种对象；同时在每个文件系统的私有数据 (如 dentry->d_fsdata 等位置) 上，使用了称为<br nodeIndex="563">
struct sysfs_dirent 的结构用于表示 /sys 中的每一个目录项。<br nodeIndex="564">
struct sysfs_dirent {<br nodeIndex="565">
atomic_t s_count;<br nodeIndex="566">
atomic_t s_active;<br nodeIndex="567">
struct sysfs_dirent *s_parent;<br nodeIndex="568">
struct sysfs_dirent *s_sibling;<br nodeIndex="569">
const char *s_name;<br nodeIndex="570">
union {<br nodeIndex="571">
struct sysfs_elem_dir s_dir;<br nodeIndex="572">
struct sysfs_elem_symlink s_symlink;<br nodeIndex="573">
struct sysfs_elem_attr s_attr;<br nodeIndex="574">
struct sysfs_elem_bin_attr s_bin_attr;<br nodeIndex="575">
};<br nodeIndex="576">
unsigned int s_flags;<br nodeIndex="577">
ino_t s_ino;<br nodeIndex="578">
umode_t s_mode;<br nodeIndex="579">
struct iattr *s_iattr;<br nodeIndex="580">
};
在上面的 kobject 对象中可以看到有向 sysfs_dirent 的指针，因此在sysfs中是用同一种 struct sysfs_dirent 来统一设备模型中的 kset/kobject/attr/attr_group.
具<br nodeIndex="585">
体在数据结构成员上， sysfs_dirent 上有一个 union<br nodeIndex="586">
共用体包含四种不同的结构，分别是目录、符号链接文件、属性文件、二进制属性文件；其中目录类型可以对应 kobject，在相应的 s_dir<br nodeIndex="587">
中也有对 kobject 的指针，因此在内核数据结构， kobject 与 sysfs_dirent 是互相引用的；
有了这些概念，再来回头看<br nodeIndex="590"><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#figure1" target="_blank" nodeIndex="591">图 1. sysfs 目录层次图</a><br nodeIndex="592">
所表达的 /sys 目录结构就是非常清晰明了:<br nodeIndex="593"><ul nodeIndex="268"><li nodeIndex="267">在 /sys 根目录之下的都是 kset，它们组织了 /sys 的顶层目录视图；</li>
<li nodeIndex="269">在部分 kset 下有二级或更深层次的 kset；</li>
<li nodeIndex="270">每个 kset 目录下再包含着一个或多个 kobject，这表示一个集合所包含的 kobject 结构体；</li>
<li nodeIndex="271">在 kobject 下有属性(attrs)文件和属性组(attr_group)，属性组就是组织属性的一个目录，它们一起向用户层提供了表示和操作这个 kobject 的属性特征的接口；</li>
<li nodeIndex="272">在 kobject 下还有一些符号链接文件，指向其它的 kobject，这些符号链接文件用于组织上面所说的 device, driver, bus_type, class, module 之间的关系；</li>
<li nodeIndex="273">不同类型如设备类型的、设备驱动类型的 kobject 都有不同的属性，不同驱动程序支持的 sysfs 接口也有不同的属性文件；而相同类型的设备上有很多相同的属性文件；</li>
</ul>
注<br nodeIndex="596">
意，此表内容是按照最新开发中的 2.6.28 内核的更新组织的，在附录资源如 LDD3 等位置中有提到 sysfs 中曾有一种管理对象称为<br nodeIndex="597">
subsys (子系统对象)，在最新的内核中经过重构认为它是不需要的，它的功能完全可以由 kset 代替，也就是说 sysfs<br nodeIndex="598">
中只需要一种管理结构是 kset，一种代表具体对象的结构是 kobject，在 kobject 下再用属性文件表示这个对象所具有的属性；<a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#main" target="_blank" nodeIndex="609"><strong nodeIndex="610">回页首</strong></a><br nodeIndex="611">
常见 sysfs 属性的功能
使用 sysfs 的关键就是掌握这些 sysfs 属性的用法，下面以一些常见的 sysfs 属性来展示它的用法；
使用设备(PCI)的 sysfs 属性文件
以一份桌面系统上的视频卡为例，列举它对应的 kobject 上的属性文件的对应用途；
一般来说，在 Linux 桌面上都有视频卡以支持 Xorg 软件包作为 XWindow 服务器来运行，因此先找到 Xorg 的进程号，查看这个进程所使用的所有文件（注意查看这个进程属性需要 root 用户权限）；<br nodeIndex="620">
# ps xfa |grep Xorg<br nodeIndex="621">
2001 tty1 Ss+ 2:24 \_ /usr/bin/Xorg :0 -nr -verbose -auth \<br nodeIndex="622">
/var/run/gdm/auth-for-gdm-NPrkZK/database -nolisten tcp vt1<br nodeIndex="623">
# lsof -nP -p 2001<br nodeIndex="624">
Xorg 2001 root mem REG 8,3 617732 231033 \<br nodeIndex="625">
/usr/lib/xorg/modules/drivers/sis_drv.so<br nodeIndex="626">
[...]<br nodeIndex="627">
Xorg 2001 root mem REG 0,0 134217728 5529 \<br nodeIndex="628">
/sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/resource0<br nodeIndex="629">
Xorg 2001 root mem REG 0,0 131072 5531 \<br nodeIndex="630">
/sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/resource1<br nodeIndex="631">
[...]<br nodeIndex="632">
Xorg 2001 root 7u REG 0,0 256 5504 \<br nodeIndex="633">
/sys/devices/pci0000:00/0000:00:00.0/config<br nodeIndex="634">
Xorg 2001 root 8u unix 0xdbe66000 0t0 8756 socket<br nodeIndex="635">
Xorg 2001 root 9u REG 0,0 256 5528 \<br nodeIndex="636">
/sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/config
注意到此 Xorg 服务器是以内存映射 (mem)<br nodeIndex="639">
的形式打开了 "/sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/resource0" 和<br nodeIndex="640">
"/sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/resource1"<br nodeIndex="641">
，同时以文件读写形式 (7u,9u) 打开了 "/sys/devices/pci0000:00/0000:00:00.0/config" 和<br nodeIndex="642">
"/sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/config"
事<br nodeIndex="645">
实上， PCI 设备对应的 kobject 目录下的 config 正是代表PCI设备的“配置空间”，对于普通 PCI<br nodeIndex="646">
(非PCI-E)设备而言，其配置空间大小一般是 256字节，这个空间可以使用十六进制工具 dump 出来，如下。(有关 PCI<br nodeIndex="647">
设备本身的三种地址空间，请参考附录 LDD3)<br nodeIndex="648">
# hexdump -C /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/config<br nodeIndex="649">
00000000 39 10 30 63 03 00 30 02 00 00 00 03 00 00 00 80 |9.0c..0.........|<br nodeIndex="650">
00000010 08 00 00 d8 00 00 00 e1 01 d0 00 00 00 00 00 00 |................|<br nodeIndex="651">
00000020 00 00 00 00 00 00 00 00 00 00 00 00 19 10 30 1b |..............0.|<br nodeIndex="652">
00000030 00 00 00 00 40 00 00 00 00 00 00 00 00 00 00 00 |....@...........|<br nodeIndex="653">
00000040 01 50 02 06 00 00 00 00 00 00 00 00 00 00 00 00 |.P..............|<br nodeIndex="654">
00000050 02 00 30 00 0b 02 00 ff 00 00 00 00 00 00 00 00 |..0.............|<br nodeIndex="655">
00000060 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................|<br nodeIndex="656">
*<br nodeIndex="657">
00000100
这<br nodeIndex="660">
个空间正好是 256字节大小，熟悉 PCI 的人们还可以知道，从 PCI 配置空间可以读到有关此 PCI<br nodeIndex="661">
设备的很多有用信息，如厂商代码，设备代码，IRQ 号码等；前四个字节 0x39 0x10 0x30 0x63 就是按小端(little<br nodeIndex="662">
endian)存放的2个短整数，因此其 PCI 厂商号码和 PCI 设备号码分别是 0x1039 和 0x6330<br nodeIndex="663">
# lspci -v -d 1039:6330<br nodeIndex="664">
01:00.0 VGA compatible controller: Silicon Integrated Systems [SiS] 661/741/760 PCI/AGP \<br nodeIndex="665">
or 662/761Gx PCIE VGA Display Adapter (prog-if 00 [VGA controller])<br nodeIndex="666">
Subsystem: Elitegroup Computer Systems Device 1b30<br nodeIndex="667">
Flags: 66MHz, medium devsel<br nodeIndex="668">
BIST result: 00<br nodeIndex="669">
Memory at d8000000 (32-bit, prefetchable) [size=128M]<br nodeIndex="670">
Memory at e1000000 (32-bit, non-prefetchable) [size=128K]<br nodeIndex="671">
I/O ports at d000 [size=128]<br nodeIndex="672">
Capabilities: [40] Power Management version 2<br nodeIndex="673">
Capabilities: [50] AGP version 3.0
在 PCI 设备上除了有 config 是配置空间对用户的接口以外，还有 resource{0,1,2,...} 是资源空间，对应着 PCI 设备的可映射内存空间；此外 PCI 设备还提供了很多接口，全部列表如下：<br nodeIndex="676">
# ls -lU /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/<br nodeIndex="677">
总计 0<br nodeIndex="678">
-rw-r--r-- 1 root root 4096 12-09 00:28 uevent<br nodeIndex="679">
-r--r--r-- 1 root root 4096 12-09 00:27 resource<br nodeIndex="680">
-r--r--r-- 1 root root 4096 12-09 00:27 vendor<br nodeIndex="681">
-r--r--r-- 1 root root 4096 12-09 00:27 device<br nodeIndex="682">
-r--r--r-- 1 root root 4096 12-09 00:28 subsystem_vendor<br nodeIndex="683">
-r--r--r-- 1 root root 4096 12-09 00:28 subsystem_device<br nodeIndex="684">
-r--r--r-- 1 root root 4096 12-09 00:27 class<br nodeIndex="685">
-r--r--r-- 1 root root 4096 12-09 00:27 irq<br nodeIndex="686">
-r--r--r-- 1 root root 4096 12-09 00:28 local_cpus<br nodeIndex="687">
-r--r--r-- 1 root root 4096 12-09 00:28 local_cpulist<br nodeIndex="688">
-r--r--r-- 1 root root 4096 12-09 00:28 modalias<br nodeIndex="689">
-rw------- 1 root root 4096 12-09 00:28 enable<br nodeIndex="690">
-rw-r--r-- 1 root root 4096 12-09 00:28 broken_parity_status<br nodeIndex="691">
-rw-r--r-- 1 root root 4096 12-09 00:28 msi_bus<br nodeIndex="692">
lrwxrwxrwx 1 root root 0 12-09 00:28 subsystem -> ../../../../bus/pci<br nodeIndex="693">
drwxr-xr-x 2 root root 0 12-09 00:28 power<br nodeIndex="694">
-rw-r--r-- 1 root root 256 12-08 23:03 config<br nodeIndex="695">
-rw------- 1 root root 134217728 12-08 23:03 resource0<br nodeIndex="696">
-rw------- 1 root root 134217728 12-09 00:28 resource0_wc<br nodeIndex="697">
-rw------- 1 root root 131072 12-08 23:03 resource1<br nodeIndex="698">
-rw------- 1 root root 128 12-09 00:28 resource2<br nodeIndex="699">
-r-------- 1 root root 0 12-09 00:28 rom
可<br nodeIndex="702">
以看到很多其它属性文件，这些属性文件的权限位也都是正确的，有 w 权限位的才是可以写入。其中大小为<br nodeIndex="703">
4096字节的属性一般是纯文本描述的属性，可以直接 cat 读出和用 echo 字符串的方法写入；其它非<br nodeIndex="704">
4096字节大小的一般是二进制属性，类似于上面的 config 属性文件；关于纯文本属性和二进制属性，在下文<br nodeIndex="705"><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/" target="_blank" nodeIndex="706">编程实践：添加sysfs支持</a><br nodeIndex="707">
一节会进一步说明。<br nodeIndex="708"><ul nodeIndex="275"><li nodeIndex="274">从<br nodeIndex="709">
vendor, device, subsystem_vendor, subsystem_device, class, resource<br nodeIndex="710">
这些只读属性上分别可以读到此 PCI 设备的厂商号、设备号、子系统厂商号、子系统设备号、PCI类别、资源表等，这些都是相应 PCI<br nodeIndex="711">
设备的属性，其实就是直接从 config 二进制文件读出来，按照配置空间的格式读出这些号码；</li>
<li nodeIndex="276">使用 enable 这个可写属性可以禁用或启用这个 PCI 设备，设备的过程很直观，写入1代表启用，写入0则代表禁用；</li>
<li nodeIndex="277">subsystem 和 driver 符号链接文件分别指向对应的 sysfs 位置；(这里缺少 driver 符号链接说明这个设备当前未使用内核级的驱动程序)</li>
<li nodeIndex="278">resource0,<br nodeIndex="712">
resource0_wc, resource1, resource2 等是从"PCI 配置空间"解析出来的资源定义段落分别生成的，它们是<br nodeIndex="713">
PCI 总线驱动在 PCI 设备初始化阶段加上去的，都是二进制属性，但没有实现读写接口，只支持 mmap 内存映射接口，尝试进行读写会提示<br nodeIndex="714">
IO 错误，其中 _wc 后缀表示 "合并式写入(write combined)" ，它们用于作应用程序的内存映射，就可以访问对应的 PCI<br nodeIndex="715">
设备上相应的内存资源段落；</li>
</ul>
有了 PCI 核心对 sysfs<br nodeIndex="718">
的完善支持，每个设备甚至不用单独的驱动程序，如这里的 "0000:01:00.0" 不需要一个内核级的驱动程序，有了 PCI<br nodeIndex="719">
核心对该设备的配置空间发现机制，可以自动发现它的各个不同段落的资源属性，在 Xorg 应用程序中可以直接以<br nodeIndex="720">
"/usr/lib/xorg/modules/drivers/sis_drv.so"<br nodeIndex="721">
这个用户空间的驱动程序对其进行映射，就可以直接操作此视频卡了；
有了这一个 PCI 设备的示例可以知道，有了一个 PCI 设备的 /sys/devices/ 设备对象，去访问它的各项属性和设置属性都非常简单。
使用 uevent
在<br nodeIndex="728">
sysfs 下的很多 kobject 下都有 uevent 属性，它主要用于内核与 udev (自动设备发现程序)之间的一个通信接口；从<br nodeIndex="729">
udev 本身与内核的通信接口 netlink 协议套接字来说，它并不需要知道设备的 uevent 属性文件，但多了 uevent<br nodeIndex="730">
这样一个接口，可用于 udevmonitor 通过内核向 udevd (udev 后台程序)发送消息，也可用于检查设备本身所支持的<br nodeIndex="731">
netlink 消息上的环境变量，这个特性一般用于开发人员调试 udev 规则文件， udevtrigger 这个调试工具本身就是以写各设备的<br nodeIndex="732">
uevent 属性文件实现的。
这些 uevent 属性文件一般都是可写的，其中 /sys/devices/ 树下的很多 uevent 属性在较新内核下还支持可读：<br nodeIndex="735">
# find /sys/ -type f -name uevent -ls<br nodeIndex="736">
11 0 -rw-r--r-- 1 root root 4096 12月 12 21:10 \<br nodeIndex="737">
/sys/devices/platform/uevent<br nodeIndex="738">
1471 0 -rw-r--r-- 1 root root 4096 12月 12 21:10 \<br nodeIndex="739">
/sys/devices/platform/pcspkr/uevent<br nodeIndex="740">
3075 0 -rw-r--r-- 1 root root 4096 12月 12 21:10 \<br nodeIndex="741">
/sys/devices/platform/vesafb.0/uevent<br nodeIndex="742">
3915 0 -rw-r--r-- 1 root root 4096 12月 12 21:10 \<br nodeIndex="743">
/sys/devices/platform/serial8250/uevent<br nodeIndex="744">
3941 0 -rw-r--r-- 1 root root 4096 12月 12 21:10 \<br nodeIndex="745">
/sys/devices/platform/serial8250/tty/ttyS2/uevent<br nodeIndex="746">
3950 0 -rw-r--r-- 1 root root 4096 12月 12 21:10 \<br nodeIndex="747">
/sys/devices/platform/serial8250/tty/ttyS3/uevent<br nodeIndex="748">
5204 0 -rw-r--r-- 1 root root 4096 12月 12 21:10 \<br nodeIndex="749">
/sys/devices/platform/i8042/uevent<br nodeIndex="750">
[...]<br nodeIndex="751">
912 0 -rw-r--r-- 1 root root 4096 12月 12 21:17 \<br nodeIndex="752">
/sys/devices/pci0000:00/0000:00:02.5/uevent<br nodeIndex="753"><p nodeIndex="754">
[...]
</p><p brd="1" nodeIndex="279"><br nodeIndex="755">
上<br nodeIndex="756">
面截取的最后一个是 SCSI 硬盘控制器设备的 uevent 属性文件，这些 /devices/ 属性文件都支持写入，当前支持写入的参数有<br nodeIndex="757">
"add","remove","change","move","online","offline"。如，写入 "add"，这样可以向<br nodeIndex="758">
udevd 发送一条 netlink 消息，让它再重新一遍相关的 udev 规则文件；这个功能对开发人员调试 udev 规则文件很有用。<br nodeIndex="759">
# echo add > /sys/devices/pci0000:00/0000:00:02.5/uevent</p>
<p brd="1" nodeIndex="280"><br nodeIndex="760">
使用驱动(PCI)的 sysfs 属性文件， bind, unbind 和 new_id
在设备驱动 /sys/bus/*/driver/... 下可以看到很多驱动都有 bind, unbind, new_id 这三个属性，<br nodeIndex="763">
# find /sys/bus/*/drivers/ -name bind -ls<br nodeIndex="764">
...
每<br nodeIndex="767">
一个设备驱动程序在程序内以某种方式注明了可用于哪些硬件，如所有的 PCI 驱动都使用 MODULE_DEVICE_TABLE 声明了所能驱动的<br nodeIndex="768">
PCI 硬件的 PCI 设备号。但驱动程序不能预知未来，未来生产的新的硬件有可能兼容现有硬件的工作方式，就还可以使用现有硬件驱动程序来工作。在<br nodeIndex="769">
bind 和 unbind 发明以前，这种情况除了修改 PCI 设备驱动程序的 DEVICE_TABLE<br nodeIndex="770">
段落，重新编译驱动程序，以外别无他法，在 2.6 内核上添加了 bind 和 unbind<br nodeIndex="771">
之后可以在不重新编译的情况下对设备和驱动之间进行手工方式地绑定。
而且对于有些硬件设备可以有多份驱动可用，但任何具体时<br nodeIndex="774">
刻只能有一个驱动程序来驱动这个硬件，这时可以使用 bind/unbind<br nodeIndex="775">
来强制使用和不使用哪一个驱动程序；(注意关于多种驱动程序的选择，更好的管理方法是使用 modprobe.conf 配置文件，需要重启才生效，而<br nodeIndex="776">
bind/unbind 提供的是一种临时的无需重启立即生效的途径；)
使用它们可以强制绑定某个设备使用或强制不使用某个驱动程序，操作方法就是通过 bind 和 unbind 接口。<br nodeIndex="779">
# find /sys/ -type f \( -name bind -or -name unbind -or -name new_id \) -ls<br nodeIndex="780">
69 0 -rw-r--r-- 1 root root 4096 12月 12 22:12 \<br nodeIndex="781">
/sys/devices/virtual/vtconsole/vtcon0/bind<br nodeIndex="782">
3072 0 --w------- 1 root root 4096 12月 12 22:15 \<br nodeIndex="783">
/sys/bus/platform/drivers/vesafb/unbind<br nodeIndex="784">
[...]<br nodeIndex="785">
6489 0 --w------- 1 root root 4096 12月 12 22:09 \<br nodeIndex="786">
/sys/bus/pci/drivers/8139too/unbind<br nodeIndex="787">
6490 0 --w------- 1 root root 4096 12月 12 22:09 \<br nodeIndex="788">
/sys/bus/pci/drivers/8139too/bind<br nodeIndex="789">
6491 0 --w------- 1 root root 4096 12月 12 22:15 \<br nodeIndex="790">
/sys/bus/pci/drivers/8139too/new_id
这个结果中特别提到了 8139too 这份驱动程序的这三个属性文件，<br nodeIndex="793">
# find /sys/bus/pci/drivers/8139too/ -ls<br nodeIndex="794">
6435 0 drwxr-xr-x 2 root root 0 12月 12 22:08 \<br nodeIndex="795">
/sys/bus/pci/drivers/8139too/<br nodeIndex="796">
6436 0 lrwxrwxrwx 1 root root 0 12月 12 22:08 \<br nodeIndex="797">
/sys/bus/pci/drivers/8139too/0000:00:0e.0 -> ../../../../devices/pci0000:00/0000:00:0e.0<br nodeIndex="798">
6485 0 lrwxrwxrwx 1 root root 0 12月 12 22:08 \<br nodeIndex="799">
/sys/bus/pci/drivers/8139too/module -> ../../../../module/8139too<br nodeIndex="800">
6488 0 --w------- 1 root root 4096 12月 12 22:08 \<br nodeIndex="801">
/sys/bus/pci/drivers/8139too/uevent<br nodeIndex="802">
6489 0 --w------- 1 root root 4096 12月 12 22:08 \<br nodeIndex="803">
/sys/bus/pci/drivers/8139too/unbind<br nodeIndex="804">
6490 0 --w------- 1 root root 4096 12月 12 22:08 \<br nodeIndex="805">
/sys/bus/pci/drivers/8139too/bind<br nodeIndex="806">
6491 0 --w------- 1 root root 4096 12月 12 22:08 \<br nodeIndex="807">
/sys/bus/pci/drivers/8139too/new_id<br nodeIndex="808">
# echo 0000:00:0e.0 > /sys/bus/pci/drivers/8139too/unbind<br nodeIndex="809">
-bash: echo: write error: 没有那个设备<br nodeIndex="810">
# ip addr<br nodeIndex="811">
1: lo: mtu 16436 qdisc noqueue state UNKNOWN<br nodeIndex="812">
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br nodeIndex="813">
inet 127.0.0.1/8 scope host lo<br nodeIndex="814">
2: eth0: mtu 1500 qdisc pfifo_fast state \<br nodeIndex="815">
UNKNOWN qlen 1000<br nodeIndex="816">
link/ether 00:14:2a:d1:16:72 brd ff:ff:ff:ff:ff:ff<br nodeIndex="817">
inet 192.168.1.102/24 brd 192.168.1.255 scope global eth0<br nodeIndex="818">
3: bond0: mtu 1500 qdisc noop state DOWN<br nodeIndex="819">
link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff<br nodeIndex="820">
# echo -n 0000:00:0e.0 > /sys/bus/pci/drivers/8139too/unbind<br nodeIndex="821">
# ip addr<br nodeIndex="822">
1: lo: mtu 16436 qdisc noqueue state UNKNOWN<br nodeIndex="823">
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br nodeIndex="824">
inet 127.0.0.1/8 scope host lo<br nodeIndex="825">
3: bond0: mtu 1500 qdisc noop state DOWN<br nodeIndex="826">
link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff<br nodeIndex="827">
# echo -n 0000:00:0e.0 > /sys/bus/pci/drivers/8139too/bind<br nodeIndex="828">
# ip addr<br nodeIndex="829">
1: lo: mtu 16436 qdisc noqueue state UNKNOWN<br nodeIndex="830">
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br nodeIndex="831">
inet 127.0.0.1/8 scope host lo<br nodeIndex="832">
3: bond0: mtu 1500 qdisc noop state DOWN<br nodeIndex="833">
link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff<br nodeIndex="834">
4: eth0: mtu 1500 qdisc noop state DOWN qlen 1000<br nodeIndex="835">
link/ether 00:14:2a:d1:16:72 brd ff:ff:ff:ff:ff:ff
这一段操作过程演示了如何对 PCI 设备 "0000:00:0e.0" 强制取消绑定 "8139too" 驱动和强制绑定 "8139too" 驱动：<br nodeIndex="838"></p>
<ul nodeIndex="282"><li nodeIndex="281">对 unbind 属性写入总线号码(bus_id)即是强制取消绑定；</li>
<li nodeIndex="283">对 bind 属性写入总线号码(bus_id)即是强制绑定；</li>
</ul>
注意，它要求的写入的是总线号码，对应于PCI设备的总线号码是按照 "domain(4位):bus(2位):slot(2位):function号(不限)" 的方式组织，是可以从其设备 kobject 节点上找到，而其它类型的总线有各自不同的规则；
请<br nodeIndex="843">
特别注意： 在这一个例子中， "echo 0000:00:0e.0 ><br nodeIndex="844">
/sys/bus/pci/drivers/8139too/unbind" 这第一个写入命令以 "No such device"<br nodeIndex="845">
为错误退出，而后续的 "echo -n" 命令则可以成功。这是因为内核在对总线号码进行匹配时过于严格了，通常的 "echo"<br nodeIndex="846">
命令写入一个字符串会以一个换行符结束输出，内核所接收到的是带有这个换行符的 bus_id 字符串，将它与内核数据结构中的真正的 bus_id<br nodeIndex="847">
字符串相比较，当然不能找到；所幸的是，这个问题在最新的 2.6.28<br nodeIndex="848">
开发中的内核上已已经解决，它将这个比较函数改为一个特殊实现的字符串比较，自动忽略结尾处的换行符，在 2.6.28-rc6<br nodeIndex="849">
内核上测试，不带"-n"参数的 echo 命令已经可以写入成功。
而 new_id<br nodeIndex="852">
属性文件也可以以另一种途径解决新的设备号问题：它是一个只写的驱动属性，可用于向其中写新的设备号。它支持写入 2至7个十六进制整形参数，分别代表<br nodeIndex="853">
vendor, device, subvendor, subdevice, class, class_mask, driver_data<br nodeIndex="854">
最少为 2个是因为一个 PCI设备主要以厂商号(vendor)和设备号(device)所唯一标定，其它 5个参数如果不输入则缺省值为<br nodeIndex="855">
PCI_ANY_ID(0xffff)。<br nodeIndex="856">
5441 0 --w------- 1 root root 4096 12月 14 18:15 \<br nodeIndex="857">
/sys/bus/pci/drivers/8139too/new_id
从 8139too 驱动上可以看到它当前所静态支持的设备号码列表，其中包括当前系统中的设备 10ec:8139, 假设未来有一款 8140 设备也满足 8139 设备的硬件通讯协议，于是可以使用 8139too 驱动程序来驱动它，操作如下<br nodeIndex="860">
# echo '10ec 8140' > /sys/bus/pci/drivers/8139too/new_id
这在不更新驱动程序的情况下调试设备很有用处。
使用 scsi_host 的 scan 属性
在具有使用 SCSI 总线连接的主机上，与 PCI类似的是也采用四个号码作为一组来描述一个设备，其中位于最顶层的是 scsi_host。
我们从设备类别 /class/为起点来探索：<br nodeIndex="869">
# ls -lU /sys/class/scsi_host<br nodeIndex="870">
总计 0<br nodeIndex="871">
lrwxrwxrwx 1 root root 0 12-13 01:59 host0 -> \<br nodeIndex="872">
../../devices/pci0000:00/0000:00:02.5/host0/scsi_host/host0<br nodeIndex="873">
lrwxrwxrwx 1 root root 0 12-13 01:59 host1 -> \<br nodeIndex="874">
../../devices/pci0000:00/0000:00:02.5/host1/scsi_host/host1
注意这是 2.6.27 内核的最新变化，在 /sys/class/ 下的都改为符号链接，真实的 kobject 都存在于 /sys/devices/ 中；我们这里探索其中的 host0 这个 SCSI 控制器：<br nodeIndex="877">
# readlink -f /sys/class/scsi_host/host0<br nodeIndex="878">
/sys/devices/pci0000:00/0000:00:02.5/host0/scsi_host/host0<br nodeIndex="879">
# ls -lU /sys/devices/pci0000:00/0000:00:02.5/host0/scsi_host/host0<br nodeIndex="880">
总计 0<br nodeIndex="881">
-rw-r--r-- 1 root root 4096 12-13 02:02 uevent<br nodeIndex="882">
lrwxrwxrwx 1 root root 0 12-13 02:02 subsystem -> ../../../../../../class/scsi_host<br nodeIndex="883">
lrwxrwxrwx 1 root root 0 12-13 02:02 device -> ../../../host0<br nodeIndex="884">
-r--r--r-- 1 root root 4096 12-13 02:02 unique_id<br nodeIndex="885">
-r--r--r-- 1 root root 4096 12-13 02:02 host_busy<br nodeIndex="886">
-r--r--r-- 1 root root 4096 12-13 02:02 cmd_per_lun<br nodeIndex="887">
-r--r--r-- 1 root root 4096 12-13 02:02 can_queue<br nodeIndex="888">
-r--r--r-- 1 root root 4096 12-13 02:02 sg_tablesize<br nodeIndex="889">
-r--r--r-- 1 root root 4096 12-13 02:02 unchecked_isa_dma<br nodeIndex="890">
-r--r--r-- 1 root root 4096 12-13 02:02 proc_name<br nodeIndex="891">
--w------- 1 root root 4096 12-13 02:02 scan<br nodeIndex="892">
-rw-r--r-- 1 root root 4096 12-13 02:02 state<br nodeIndex="893">
-rw-r--r-- 1 root root 4096 12-13 02:02 supported_mode<br nodeIndex="894">
-rw-r--r-- 1 root root 4096 12-13 02:02 active_mode<br nodeIndex="895">
-r--r--r-- 1 root root 4096 12-13 02:02 prot_capabilities<br nodeIndex="896">
-r--r--r-- 1 root root 4096 12-13 02:02 prot_guard_type<br nodeIndex="897">
drwxr-xr-x 2 root root 0 12-13 02:02 power
对这些属性文件解释如下：<br nodeIndex="900"><ul nodeIndex="285"><li nodeIndex="284"><strong nodeIndex="901">有四个 SCSI 特有的可写参数： scan,state,supported_mode,active_mode</strong>；可以向其中写入不同的参数来控制此 SCSI 控制器的各种状态；</li>
<li nodeIndex="286">其它一些可读属性用于读取这个 SCSI 控制器的一些当前值；</li>
</ul>
其<br nodeIndex="904">
中的 scan 属性文件在调试一些 SCSI 硬件驱动时很有用，它是只写的，可以写入三个至四个以空格分开的整数，用于分别指定对应的 host,<br nodeIndex="905">
channel, id, lun 进行重新搜索。且这个 scan 属性支持以"-"作为通配符，如以下命令可以执行让整个 scsi_host<br nodeIndex="906">
进行重新搜索，这个功能用于调试某些对热挺拔实现不完善的 SCSI 驱动程序很有用：<br nodeIndex="907">
# echo '- - -' >/sys/devices/pci0000:00/0000:00:02.5/host0/scsi_host/host0/scan
内核模块中的 sysfs 属性文件
以一个 8139too 模块为例解释在这个 kboject 下每一个属性的用途；<br nodeIndex="912">
# find /sys/module/8139too/ -ls<br nodeIndex="913">
6408 0 -r--r--r-- 1 root root 4096 12月 13 02:17 \<br nodeIndex="914">
/sys/module/8139too/version<br nodeIndex="915">
6412 0 drwxr-xr-x 2 root root 0 12月 13 02:17 \<br nodeIndex="916">
/sys/module/8139too/sections<br nodeIndex="917">
6433 0 drwxr-xr-x 2 root root 0 12月 13 02:17 \<br nodeIndex="918">
/sys/module/8139too/notes<br nodeIndex="919">
6434 0 -r--r--r-- 1 root root 36 12月 13 02:17 \<br nodeIndex="920">
/sys/module/8139too/notes/.note.gnu.build-id<br nodeIndex="921">
6486 0 drwxr-xr-x 2 root root 0 12月 13 02:17 \<br nodeIndex="922">
/sys/module/8139too/drivers<br nodeIndex="923">
6487 0 lrwxrwxrwx 1 root root 0 12月 13 02:17 \<br nodeIndex="924">
/sys/module/8139too/drivers/pci:8139too -> ../../../bus/pci/drivers/8139too
其<br nodeIndex="927">
中的属性文件都是只读的，用于提供信息。从 version, srcversion 上可以了解到这个模块所声明的版本号，源码版本号，<br nodeIndex="928">
refcnt 是模块引用计数， sections 属性组中有一些模块加载至内存的相应节信息， drivers/ 目录中是对所提供的驱动的链接；
因<br nodeIndex="931">
为模块是内核驱动编程的最佳选择，而一个模块有可能提供多个驱动程序，因而在未知一个设备在用哪一个驱动的情况下可以先从 /sys/module/<br nodeIndex="932">
查找相应模块的情况，再从 drivers/ 发现出真正的驱动程序。或者也可以完全反过来利用这些信息，先用 lspci/lshw 等工具找到<br nodeIndex="933">
/sys/devices/ 下的设备节点，再从其设备的 driver 链接找到 /sys/bus/*/drivers/ 下的<br nodeIndex="934">
device_driver, 再从 device_driver 下的 module 链接找到<br nodeIndex="935">
/sys/module/*/，这样就可以得到已加载模块中空间是哪一个模块在给一个设备提供驱动程序。
更多 sysfs 属性文件
以<br nodeIndex="940">
上所举的例子仅仅是一些常见的 sysfs 属性用法，实际的系统中还常常有很多其它的从未见过的 sysfs<br nodeIndex="941">
属性，因此只有举例是不够的，即使维护了一份 sysfs 属性用法参考大全也不够，未来的内核版本还会出现新的 sysfs 属性，因此还必须了解<br nodeIndex="942">
Linux 内核代码以找到实现这些属性的代码位置，以学会在没有相应属性文档的情况从内核源代码来分析其 sysfs 属性功能。<a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#main" target="_blank" nodeIndex="953"><strong nodeIndex="954">回页首</strong></a><br nodeIndex="955">
Sysfs 源码分析和编程实践
从源代码中理解 sysfs 属性的用途
更多的 sysfs 属性的功能只能靠阅读源代码来理解。还是以上文提到的 scsi_host 的 scan 属性来理解，这个功能没有任何文档上有描述，因此只能去读源代码。
在<br nodeIndex="962">
内核中， sysfs 属性一般是由 __ATTR 系列的宏来声明的，如对设备的使用 DEVICE_ATTR ，对总线使用 BUS_ATTR<br nodeIndex="963">
，对驱动使用 DRIVER_ATTR ，对类别(class)使用 CLASS_ATTR, 这四个高级的宏来自于<br nodeIndex="964">
, 都是以更低层的来自<br nodeIndex="965">
中的 __ATTR/__ATRR_RO 宏实现； 因此我们在内核源码树中相应位置 drivers/scsi/<br nodeIndex="966">
找到这几个宏的使用情况，可以得到在 drivers/scsi/scsi_sysfs.c 中：<br nodeIndex="967">
static ssize_t<br nodeIndex="968">
store_scan(struct device *dev, struct device_attribute *attr,<br nodeIndex="969">
const char *buf, size_t count)<br nodeIndex="970">
{<br nodeIndex="971">
struct Scsi_Host *shost = class_to_shost(dev);<br nodeIndex="972">
int res;<br nodeIndex="973">
res = scsi_scan(shost, buf);<br nodeIndex="974">
if (res == 0)<br nodeIndex="975">
res = count;<br nodeIndex="976">
return res;<br nodeIndex="977">
};<br nodeIndex="978">
static DEVICE_ATTR(scan, S_IWUSR, NULL, store_scan);
DEVICE_ATTR<br nodeIndex="981">
宏声明有四个参数，分别是名称、权限位、读函数、写函数。这里对应的，名称是 scan,<br nodeIndex="982">
权限是只有属主可写(S_IWUSR)、没有读函数、只有写函数。因此读写功能与权限位是对应的，因为 DEVICE_ATTR<br nodeIndex="983">
把权限位声明与真正的读写是否实现放在了一起，减少了出现不一致的可能。(上文提到 /proc/scsi/scsi<br nodeIndex="984">
接口的权限位声明与其功能不对应，这与注册 proc<br nodeIndex="985">
接口的函数设计中的不一致是有关系的，权限位声明与功能实现不在代码中同一个位置，因此易出错。虽然修复 /proc/scsi/scsi<br nodeIndex="986">
的权限位错误很容易，但内核团队中多年来一直没有人发现或未有人去修正这个 BUG，应该是与 /proc/scsi/<br nodeIndex="987">
接口的过时有关，过时的功能会在未来某个内核版本中去除。)
上面的 scan 属性写入功能是在 store_scan<br nodeIndex="990">
函数中实现的，这个接口的四个参数中， buf/count 代表用户写入过来的字符串，它把 buf 进一步传给了 scsi_scan<br nodeIndex="991">
函数；如果进一步分析 scsi_scan 函数实现可以知道，它期望从 buf 中接受三个或四个整型值(也接受"-"作为通配符)，分别代表<br nodeIndex="992">
host, channel, id 三个值，(第四个整数在早期内核中曾代表 lun<br nodeIndex="993">
号码，但在较新内核中第四个数字被忽略，仅作为向后兼容保留接受四个整数)，然后对具体的 (host, channel, id)<br nodeIndex="994">
进行重新扫描以发现这个 SCSI 控制器上的设备变动。
添加 sysfs 支持
如果你正在开发的设备驱动程序中需要与用户层的接口，一般可选的方法有：<br nodeIndex="999"><table cellpadding="0" cellspacing="0" nodeIndex="1000" class=" ril_dataTable"><tbody nodeIndex="1001"><tr nodeIndex="288"><td class="t_f" id="postmessage_14107588" nodeIndex="287">
<ul nodeIndex="290"><li nodeIndex="289">注<br nodeIndex="1002">
册虚拟的字符设备文件，以这个虚拟设备上的 read/write/ioctl 等接口与用户交互；但 read/write 一般只能做一件事情，<br nodeIndex="1003">
ioctl 可以根据 cmd 参数做多个功能，但其缺点是很明显的： ioctl 接口无法直接在 Shell 脚本中使用，为了使用 ioctl<br nodeIndex="1004">
的功能，还必须编写配套的 C语言的虚拟设备操作程序， ioctl 的二进制数据接口也是造成大小端问题 (big endian与little<br nodeIndex="1005">
endian)、32位/64位不可移植问题的根源；</li>
<li nodeIndex="291">注册 proc 接口，接受用户的 read/write/ioctl 操作；同样的，一个 proc 项通常使用其 read/write/ioctl 接口，它所存在的问题与上面的虚拟字符设备的的问题相似；</li>
<li nodeIndex="292">注册 sysfs 属性；
最<br nodeIndex="1008">
重要的是，添加虚拟字符设备支持和注册 proc 接口支持这两者所需要增加的代码量都并不少，最好的方法还是使用 sysfs<br nodeIndex="1009">
属性支持，一切在用户层是可见的透明，且增加的代码量是最少的，可维护性也最好；方法就是使用<br nodeIndex="1010">
头文件提供的这四个宏，分别应用于总线/类别/驱动/设备四种内核数据结构对象上：<br nodeIndex="1011">
#define BUS_ATTR(_name, _mode, _show, _store) \<br nodeIndex="1012">
struct bus_attribute bus_attr_##_name = __ATTR(_name, _mode, _show, _store)<br nodeIndex="1013">
#define CLASS_ATTR(_name, _mode, _show, _store) \<br nodeIndex="1014">
struct class_attribute class_attr_##_name = __ATTR(_name, _mode, _show, _store)<br nodeIndex="1015">
#define DRIVER_ATTR(_name, _mode, _show, _store) \<br nodeIndex="1016">
struct driver_attribute driver_attr_##_name = \<br nodeIndex="1017">
__ATTR(_name, _mode, _show, _store)<br nodeIndex="1018">
#define DEVICE_ATTR(_name, _mode, _show, _store) \<br nodeIndex="1019">
struct device_attribute dev_attr_##_name = __ATTR(_name, _mode, _show, _store)
总线(BUS)和类别(CLASS)属性一般用于新设<br nodeIndex="1022">
计的总线和新设计的类别，这两者一般是不用的；因为你的设备一般是以PCI等成熟的常规方式连接到主机，而不会去新发明一种类型；使用驱动属性和设备属性<br nodeIndex="1023">
的区别就在于：看你的 sysfs 属性设计是针对整个驱动有效的还是针对这份驱动所可能支持的每个设备分别有效。
从头文<br nodeIndex="1026">
件中还可以找到 show/store 函数的原型，注意到它和虚拟字符设备或 proc 项的 read/write 的作用很类似，但有一点不同是<br nodeIndex="1027">
show/store 函数上的 buf/count 参数是在 sysfs 层已作了用户区/内核区的内存复制，虚拟字符设备上常见的 __user<br nodeIndex="1028">
属性在这里并不需要，因而也不需要多一次 copy_from_user/copy_to_user, 在 show/store 函数参数上的<br nodeIndex="1029">
buf/count 参数已经是内核区的地址，可以直接操作。
上面四种都是 Linux<br nodeIndex="1032">
统一设备模型所添加的高级接口，如果使用 sysfs 所提供的底层接口的话，则还有下面两个，定义来自<br nodeIndex="1033">
：(上面的总线/类别/驱动/设备四个接口都是以这里的__ATTR实现的)<br nodeIndex="1034">
#define __ATTR(_name,_mode,_show,_store) { \<br nodeIndex="1035">
.attr = {.name = __stringify(_name), .mode = _mode }, \<br nodeIndex="1036">
.show = _show, \<br nodeIndex="1037">
.store = _store, \<br nodeIndex="1038">
}<br nodeIndex="1039">
#define __ATTR_RO(_name) { \<br nodeIndex="1040">
.attr = { .name = __stringify(_name), .mode = 0444 }, \<br nodeIndex="1041">
.show = _name##_show, \<br nodeIndex="1042">
}
上<br nodeIndex="1045">
面这些宏都是在注册总线/类别/驱动/设备时作为缺省属性而使用的，在实际应用中还有一种情况是根据条件动态添加属性，如 PCI 设备上的<br nodeIndex="1046">
resource{0,1,2,...} 属性文件，因为一个 PCI 设备上的可映射资源究竟有多少无法预知，也只能以条件判断的方式动态添加上。<br nodeIndex="1047">
int __must_check sysfs_create_file(struct kobject *kobj,<br nodeIndex="1048">
const struct attribute *attr);<br nodeIndex="1049">
int __must_check sysfs_create_bin_file(struct kobject *kobj,<br nodeIndex="1050">
struct bin_attribute *attr);
这两个函数可以对一个 kobject 动态添加上文本属性或二进制属性，这也是唯一可以添加二进制属性的方法。
二进制属性与普通文本属性的区别在于：<br nodeIndex="1055"></li>
</ul><ul nodeIndex="294"><li nodeIndex="293">二进制属性 struct bin_attribute 中内嵌一个 struct attribute 结构体对象，因此具有普通属性的所有功能特征；</li>
<li nodeIndex="295">二进制属性上多一个 size 用来描述此二进制文件的大小，而普通属性文件的大小总是 4096, 准确地说，应该是一个内存页的大小，因为从当前 sysfs 内核实现来说，它分配一个内存页面来作为 (buf/count) 的缓冲区；</li>
<li nodeIndex="296">二进制属性比普通属性多内存映射(mmap)接口的支持；</li>
</ul>
编程示例，对 LDD3 一书中的 lddbus 驱动程序的 sysfs 改进
首先，这个程序本身是针对当时作者写书的年代的内核(2.6.11)而编写的，在当前的 Fedora10 系统 (2.6.27.5-117.fc10.i686) 上甚至无法编译编译通过；因此首先需要将它移植过来至少达到可运行状态；
附件的压缩包中含有修改过的 lddbus, sculld 的源代码和修改过程的四个patch:<br nodeIndex="1062"><ul nodeIndex="298"><li nodeIndex="297">第一个 0001-ldd3-examples-build-on-fedora-10-2.6.27.5-117.fc10.i.patch 是将 lddbus 和 sculld 移植到 Fedora10 内核上可运行，这其中主要是一此内核 API 的变化；</li>
<li nodeIndex="299">第<br nodeIndex="1063">
二个 0002-port-dmem-proc-entry-to-use-sysfs-entry.patch 演示了怎样将原有的 proc<br nodeIndex="1064">
接口改进成为 sysfs 属性接口的，从这个 patch 中可以看到删除的代码多而新增加的代码少，这说明对于相同的功能，使用 sysfs<br nodeIndex="1065">
编程接口的代码量更少，而且 sysfs 代码看起来也比 proc<br nodeIndex="1066">
更为整洁：打印每个设备的调试信息可以做成每个设备上分别有自己的接口，而不是统一的一个 proc 接口；设备属性文件最终出现的位置如<br nodeIndex="1067">
"/sys/devices/ldd0/sculld0/dmem"； static ssize_t sculld_show_dmem(struct device *ddev,<br nodeIndex="1068">
struct device_attribute *attr, char *buf)<br nodeIndex="1069">
{<br nodeIndex="1070">
/* 其中打印每个设备调试信息的代码复制自原proc接口 */<br nodeIndex="1071">
}<br nodeIndex="1072">
static DEVICE_ATTR(dmem, S_IRUGO, sculld_show_dmem, NULL);<br nodeIndex="1073">
static int __init sculld_register_dev(struct sculld_dev *dev, int index)<br nodeIndex="1074">
{<br nodeIndex="1075">
/* 创建此device属性文件 */<br nodeIndex="1076">
ret |= device_create_file(&dev->ldev.dev, &dev_attr_dmem);<br nodeIndex="1077">
}<br nodeIndex="1078"></li>
<li nodeIndex="300">第三个 0003-add-.gitignore.patch 是增加了 .gitignore 文件，屏蔽一些编译生成的临时文件；</li>
<li nodeIndex="301">第<br nodeIndex="1079">
四个 0004-port-qset-get-set-ioctl-to-use-sysfs-entry.patch 演示了怎样把基于 ioctl<br nodeIndex="1080">
的操作接口改进成为基于 sysfs 接口，由于原来的 ioctl 接口设置和获取 qset<br nodeIndex="1081">
信息是表示整个驱动模块级的变量，它用来控制整个驱动程序而非驱动所支持的单个的设备，因此这个 qset 属性使用 DRIVER_ATTR<br nodeIndex="1082">
来添加更为合适； ssize_t sculld_show_qset(struct device_driver *driver, char *buf)<br nodeIndex="1083">
{<br nodeIndex="1084">
return snprintf(buf, PAGE_SIZE, "%d\n", sculld_qset);<br nodeIndex="1085">
}<br nodeIndex="1086">
ssize_t sculld_store_qset(struct device_driver *driver, const char *buf,<br nodeIndex="1087">
size_t count)<br nodeIndex="1088">
{<br nodeIndex="1089">
sculld_qset = simple_strtol(buf, NULL, 0);<br nodeIndex="1090">
return count;<br nodeIndex="1091">
}<br nodeIndex="1092">
/* 声明一个权限为0644的可同时读写的driver属性 */<br nodeIndex="1093">
static DRIVER_ATTR(qset, S_IRUGO | S_IWUSR, sculld_show_qset, sculld_store_qset);<br nodeIndex="1094">
/* 创建此driver属性文件 */<br nodeIndex="1095">
result = driver_create_file(&sculld_driver.driver, &driver_attr_qset);
驱动属性最终出现如 "/sys/bus/ldd/drivers/sculld/qset" ，这里声明的是同时可读写的，权限位 0644 与其保持一致。<br nodeIndex="1098">
6446 0 -rw-r--r-- 1 root root 4096 12月 14 07:44 /sys/bus/ldd/drivers/sculld/qset<br nodeIndex="1099"></li>
</ul><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/#main" target="_blank" nodeIndex="1110"><strong nodeIndex="1111">回页首</strong></a><br nodeIndex="1112">
小结
sysfs<br nodeIndex="1115">
给应用程序提供了统一访问设备的接口，但可以看到， sysfs 仅仅是提供了一个可以统一访问设备的框架，但究竟是否支持 sysfs<br nodeIndex="1116">
还需要各设备驱动程序的编程支持；在 2.6 内核诞生 5年以来的发展中，很多子系统、设备驱动程序逐渐转向了 sysfs<br nodeIndex="1117">
作为与用户空间友好的接口，但仍然也存在大量的代码还在使用旧的 proc 或虚拟字符设备的 ioctl 方式；如果仅从最终用户的角度来说，<br nodeIndex="1118">
sysfs 与 proc 都是在提供相同或类似的功能，对于旧的 proc 代码，没有绝对的必要去做 proc 至 sysfs<br nodeIndex="1119">
的升级；因此在可预见的将来， sysfs 会与 proc, debugfs, configfs 等共存很长一段时间。<br nodeIndex="1120">
转自：http://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/</td>
</tr></tbody></table><br nodeIndex="1121"></div>