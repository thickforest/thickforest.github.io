---
layout: post
title: 学界 | Facebook研究者提出新型文本分类方法fastText：性能比肩深度学习而且速度更快
categories:
- 今日头条
tags:
---
选自 arXiv.org

作者：Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov

机器之心编译

参与：黄清纬、吴攀

![](http://p3.pstatp.com/large/b12000d80c48f893544)

摘要

本论文提出了一个简单而高效的文本分类和表征学习的方法。我们的实验证明我们的快速文本分类器 fastText 在准确度上的表现与深度学习分类器相当，并且在训练和评估上要比深度学习分类器快好几个数量级。我们可以通过使用一个标准多核 CPU 在十分钟完成对 fastText 的 10 亿多个词的训练，并在一分钟内将 50 万个句子分成 31.2 万个类别。

1. 导言

为文本分类构建准确的表征是一项重要的任务，具有广泛的应用场景，例如网页搜索、信息检索、排名和文档分类（Deerwester et al., 1990; Pang and Lee, 2008）。最近，基于神经网络的模型在计算句子的表征上越来越流行（Bengio et al., 2003; Collobert and Weston, 2008）。虽然这些模型的实际表现非常好（Kim, 2014; Zhang and LeCun, 2015; Zhang et al., 2015），但是它们在训练和测试上花的时间相对要多，这限制了它们在超大数据集上的应用。同时，简单线性模型的表现也令人印象深刻，具有很高的计算效率（Mikolov et al., 2013; Levy et al., 2015）。它们通常学习词层面的表征，这些词表征之后会组合成句子表征。在本成果中，我们提出了一个这些模型的扩展，可用来直接学习句子表征。我们证明通过纳入额外的统计数据（如使用 n-gram 包），可以缩小线性模型和深度模型之间的准确度差距，并且要比原来快好几个数量级。我们的成果与标准线性文本分类器（Joachims, 1998; McCallum and Nigam, 1998; Fan et al., 2008）密切相关。与 Wang 和 Manning（2012）相似，我们探索简单基线的动力受到了用于无监督学习词表征的模型的启发。与 Le 和 Mikolov（2014）不同，我们的方法在测试时不需要繁复的推理，这使得其学到的表征可以轻松应用到不同的问题上。我们在两个不同的任务中评估我们模型的质量，这两个任务叫做标签预测（tag prediction）和情绪分析（sentiment analysis）。



2. 模型架构

一条简单而高效的句子分类基线就是将句子表征为词袋（Bag of Words，BoW）并训练一个线性分类器，例如一个逻辑回归或支持向量机（Joachims, 1998; Fan et al., 2008）。然而，线性分类器不能在特征和类别之间共享参数，这可能会限制泛化（generalization）。一般解决这个问题的方法是将线性分类器分解成低秩矩阵（ low rank matrices）（Schutze, 1992; Mikolov et al., 2013）或使用多层神经网络（Collobert and Weston, 2008; Zhang et al., 2015）。如果用神经网络的话，信息将通过隐藏层共享。

![](http://p3.pstatp.com/large/b15000fe29041d1f3cc)

用于快速句子分类的模型架构

图 1 展示了一个有一个隐藏层的简单模型。第一个权重矩阵 W1 可以被视作某个句子的词查找表。词表征被平均成一个文本表征，然后其会被馈送入一个线性分类器。这个构架和 Mikolov et al.（2013）论文中的 cbow 模型相似，只是中间词（middle word）被替换成了标签（label）。该模型将一系列单词作为输入并产生一个预定义类的概率分布。我们使用一个 softmax 方程来计算这些概率。这种模型的训练本质上与 word2vec 相似，即：我们使用随机梯度下降和带有线性衰变学习率的反向传播（Rumelhart et al., 1986）。我们的模型是在多个 CPU 上以异步的方式训练的。

2.1 分层 softmax

当目标的数量太大时，线性分类器的计算会变得十分昂贵。更确切地说，计算复杂度是 O(Kd) ，其中 K 是目标的数量，d 是隐藏层的维度。为了改善我们的运行时间，我们使用了一个基于霍夫曼编码树（Mikolov et al., 2013）的分层 softmax （Goodman, 2001）。在训练过程中，计算复杂度会下降到

![](http://p3.pstatp.com/large/b15000fe292ac2db457)

在这个树中，目标是它的树叶。在测试的时候，该分层 softmax 对于搜索最可能的类也是有利的。每个结点与从根到该节点的路径的概率相关联。如果该结点的深度是 l+1，其父结点是 n1,... nl，那么它的概率是 

![](http://p3.pstatp.com/large/b1100028734cb9dc81c)

这意味着一个结点的概率总是比它的父结点小。使用深度优先搜索（depth first search）来历遍树并跟踪最大概率的树叶，使我们可以剪去任何与较小概率相联的分枝。在实践中，我们观察到测试时间的复杂度下降到 

该方法能使用二叉堆（ binary heap）进行扩展，以在 O(log(T)) 的成本下计算出 T 个最可能的目标。

2.2 n-gram 特征

词袋（BoW）中的词顺序是不变的，但是明确考虑该顺序的计算成本通常十分高昂。作为替代，我们使用 n-gram 包（bag of n-gram）作为额外特征来获取关于局部词顺序（local word order）的部分信息。在实践中这非常高效，而且通常能得到可以与明确使用词顺序的方法所得到的相比的结果（Wang and Manning, 2012）。我们通过一个哈希（hashing）技巧维持了快速且有效的 n-gram 映射（Weinberger et al., 2009），这个哈希技巧带有与 Mikolov et al.（2011）论文中相同的哈希函数和 10M bins（如果我们仅使用 bigram 的话），否则就需要 100M bin.

3. 实验过程

实验的具体过程和数据可点击文后「阅读原文」下载

4. 讨论和总结

在本成果中，我们开发了 fastText，从而将 word2vec 成果扩展用来分类句子和文档。和 word2vec 中的无监督训练的词向量不同，我们的词特征可以一起被平均而组成好的句子表征。在几个任务中，我们得到了与最近提出的深度学习启发的方法相当的表现，并且还获得了极大的速度提升。虽然深度神经网络在理论上比浅度模型有强得多的表征能力，但目前还不清楚简单的文本分类问题（如情绪分析）是否是评估它们的正确选择。我们将会公布我们的代码，这样研究社区就可以在我们的基础上继续前进。

©本文由机器之心编译，转载请联系本公众号获得授权。

✄------------------------------------------------

加入机器之心（全职记者/实习生）：hr@almosthuman.cn

投稿或寻求报道：editor@almosthuman.cn

广告&商务合作：bd@almosthuman.cn

点击「阅读原文」，下载 PDF↓↓↓